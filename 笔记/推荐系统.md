李亚东->东哥

电话->15901108226(同步微信)

growingIO:  https://accounts.growingio.com/login

诸葛IO : https://zhugeio.com/demo

易观：https://demo.analysysdata.com/dashboard/detail/1584

神策：https://www.sensorsdata.cn/demo/demo.html

kafka的面试题总结：https://mp.weixin.qq.com/s/XdriIP7FqYn-87hmqc32qA



回顾

![1603761804609](%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/1603761804609.png)





# 1 推荐系统的背景

## 1.1 背景

为应用带来价值

`数学，编程，机器学习等`



## 1.2 俗语

召回和排序

流程：第一阶段做`召回`模型，第二阶段将`召回`的结果输入排序模型做`排序`，之后按排序结果推给用户就可以了(`推给用户之前还会加一层用户已读已推过滤层避免重复推荐`)。

召回定义：从全量数据集合中找出正确的数据集。

评估召回模型/算法的指标：召回率和精确率

- 召回率`(Recall)` = 检索到的相关的内容数量 / 系统中所有相关的内容数量  `在信息检索领域也叫查全率`
- 精确率`(Precision)`= 检索到的相关的内容数量 / 检索到所有内容数量 ``在信息检索领域也叫查准率`

举个例子来解释一下，比如系统一共`100`个样本(你可以理解成Google只收录了100条信息)，其中正样本(理解为涵盖`推荐系统召回`这个短语语义的样本) `60`个，负样本(不涵盖`推荐系统召回`这个短语语义的样本)`40`个。我们搜索`推荐系统召回`这个短语，系统通过算法给我们返回了`50`条系统认为相关信息，但是其中`40`个是正样本，`10`个是负样本。这种情况下系统的`召回率`和`精确率`是多少呢？

- $召回率 = 40 / 60 = 2/3$
- $精确率 = 40 / 50  = 4/5 $

我们希望系统的准确率和召回率都是越高越好，但是现实的实际情况是这两个指标总是相互制约的，如果我们系统召回率很高，那么精确率就会降低，反之亦然。



召回：一般都是多路召回算法，最后将多路召回算法进行合并，就是一个召回结果集。



Ranking(排序)：

定义：将多路召回算法的结果集进行排序(按照得分)。如果不排序，多路召回结果怎么返回给用户？？？



## 1.3 项目需求

### 1.3.1 推荐场景

- 实现推荐页的内容推荐需求，提供API即可，接口支持传入`用户ID` ,返回该用户感兴趣的`Item[即文章ID]`。要求召回策略使用`ItemCF`,`ALS`,`Hot` 三种召回策略即可，排序模型使用`GBDT+LR`
- 实现详情页的同类物品推荐，即用户阅读完一篇文章后，在该篇文章的底部推荐与该文章内容同类的文章。依然提供API，接口支持传入`Item[即文章ID]`, 返回与该文章内容同类的`Item 列表`



### 1.3.2 概要

推荐系统项目构建于用户画像项目之上的，请务必确保已经掌握用户画像项目，并有了需求中提到的必须的数据表，才能继续推荐系统项目的学习。

- 本项目工程组件： `Spark MLlib + SpringBoot + Redis +HBase+ Milvus`
- 本项目涉及算法:  `Word2Vec` + `ItemCF`+`ALS`+`GBDT`+`LR`+`BloomFilter`
- 监控架构依然采用：`Prometheus` +` Grafana` +` IM`
- 本项目的组件监控：` 推荐API`+`HBase[Metric]`+`Prometheus` +` Grafana` +`Supervisor`+ `IM`

### 1.3.3 架构图

![](http://pcmyp.oss-cn-beijing.aliyuncs.com/markdown/2020-04-21-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BC%81%E4%B8%9A%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%20-2-.jpg?x-oss-process=style/wm_qf)



### 1.3.4 召回架构的核心

1、评分数据怎么打？？人为指定某个行为评分或者直接使用隐式数据反馈

2、物品基础标签中---非数值类型的标签怎么处理？？数值类型标签怎么消除量级？？



### 1.3.5 排序架构核心

1、计算对召回的物品进行点击率的预测？？

2、将算法模型进行跨平台部署？？PMML



# 2 项目部署实施

## 2.1 Collaborative Filtering(协同过滤)

协同过滤推荐一般分为三种类型`1.基于用户的的协同过滤[UserCF] 2. 基于物品的协同过滤[ItemCF] 3.基于模型的协同过滤[ModelCF] `我们分别对这三种类型做一个简单说明

- 基于用户`(UserBased)`的协同，表达的含义是将与一个用户相似的用户喜欢的物品推荐给这个用户。这里需要做的是首先找到和一个用户相似的用户，然后找到相似用户喜欢的物品`(可以理解成发生过特定行为或者评分高的物品)`，预测这个用户对这些物品的评分，找到评分高的若干物品推给该用户就可以了。
- 基于物品`(ItemBased)`的协同，表达的含义是找到与一个用户喜欢的物品`(可以理解成发生过特定行为或者评分高的物品)`相似的物品推荐给这个用户。 这里需要做的是首先找到一个物品的相似物品，然后将这些相似的物品打分排序，找到评分高的若干物品推荐给用户就可以了。
- 基于模型`(ModelBased)`的协同，这个相对就比较复杂了，也是目前较为主流的协同过滤算法，其相关算法完全可以写本书了。我们这里对其思想做一个了解即可。假设我们有N个用户，M个物品，只有部分用户和部分物品之间是有联系的`(这一点很容易想到，比如用户不能看完我们文章库的所有内容，用户也不可能对商品库的所有商品都有收藏等购买行为)`，其他部分都是空白`(这里我们把N用户和M个物品，看成一个N*M的矩阵)`，我们可以想象这个矩阵是一个稀疏矩阵，大部分是空白的，我们现在要做的就是，用这些矩阵中已有的数据，使用某些算法去将空白区域的值给填补上`(就像数独游戏一样，已知一些数据，填补空白数据)`，这样我们就相当于有了每一个用户对每一个物品数据，我们按照这个数据排序，推荐TOPN的物品给用户就可以了。这就是基于模型协同过滤的核心思想。 想要给空白处填补数据可以使用很多种算法，比如`矩阵分解，分类，回归，神经网络等等`，我们项目中只讲`矩阵分解`这种类型的算法的其中一个`ALS[交替最小二乘法]`算法。



## 2.2 相似度

对于`ItemCF`我们上文已经简单介绍过，这里我们再做进一步的分析。要做`ItemCF`我们首先就是要根据用户对物品的行为数据， 找到物品与物品之间的相似度。要计算相似度，我们先来了解两个相似度的计算公式

- 余弦相似度

  $sim_{X,Y}=cos\theta=\frac{XY}{||X||||Y||}=\frac{ \sum_{i=1}^n(x_iy_i)}{\sqrt{\sum_{i=1}^n(x_i)^2}*\sqrt{\sum_{i=1}^n(y_i)^2}}$

  其中$x_i$ 表示第`i` 个用户对物品`x` 的评分，$y_i$表示第`i`个用户对物品`y`的评分

- 同现相似度

  $w(x,y)=\frac{|N(x)\cap{N(y)}|}{|N(x)|}$

  $N(x)$ 表示喜欢物品`x` 的用户数，$N(y)$  表示喜欢物品`y` 的用户数据。 $|N(x)\cap{N(y)}|$ 表示同时喜欢`x`和`y` 的用户数。 上边的公式表达的意思是如果对`x`物品刚兴趣的用户，同时对`y`物品刚兴趣的程度。如果对`x`感兴趣的用户同时都对`y`感兴趣，那么$w(x,y)=1$，这时`x`,`y`两个物品相似度最高。

  但是这个相似度计算公式会有些问题，比如当`y`是一个热门物品的时候，就会导致$w(x,y)$很大，总是接近于`1`,这就造成任何物品和热门物品都有很大的相似度，因此我们要对热门物品做一个惩罚，修正一个我们的公式如下

  $w(x,y)=\frac{|N(x)\cap{N(y)}|}{\sqrt{|N(x)||N(y)|}}$

  这个式子惩罚了物品`y`的权重，因此减轻了热门物品和很多物品相似的可能性。

- 除了我们上述说的两个相似度计算公式外，还有其他很多相似计算公式，比如`欧几里得相似度,皮尔逊相似度`,`Tanimoto 相似度(Jaccard 系数)` 等，我们这里不做过多介绍。



余弦相似度求解过程：

![1603782136498](%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/1603782136498.png)



## 2.3 文章相似度计算

计算文章相似度需要数据：用户对文章行为数据(event)和文章的基础数据(news)



```sql
1、查询用户对文章的行为数据
(用户，物品，行为,行为发生的日期)` 这四列基础数据，存储到dwb_news.user_news_action.

-- 连接presto，执行如下SQL, 我们将此表建立到`dwb`层,命名为`dwb_news.user_article_action`
-- 此SQL提取用户对文章的行为数据，包括（点击，分享，评论，收藏，点赞）五个行为
create table dwb_news.user_article_action
comment 'user article action data '
with (format = 'ORC')
as 
with t1 as (
select distinct_id as uid , 
article_id as aid, 
case when (event='AppPageView' and element_page='内容详情页') then '点击' 
else action_type end as action,
logday as action_date
from ods_news.event
where event in ('NewsAction','AppPageView')
and logday>=format_datetime(now()- interval '60' day,'yyyyMMdd') 
and  logday< format_datetime(now(),'yyyyMMdd')
and article_id <>''
)
select uid,aid,action,max(action_date)as action_date from t1 where action<>'' 
group by uid,aid,action;


-- 生成数据后执行查询，看数据是否OK
select uid,
aid,
action, 
action_date from dwb_news.user_article_action limit 10;
```

## 2.3 相似度基础数据表示形式

```
* 我们最终需要的数据，应该是`(用户，物品，评分)`这样的带有评分的数据，或者是`(用户，物品)` 这样没有评分的数据，表示用户对物品有过行为，类似我们上述举的例子，有过行为的物品都是1，没有行为的都是0。 这两种类型的数据格式都可以作为我们算法的原始数据作为输入。 现在的问题是，如果将我们抽取`dwb_news.user_article_action`表中的数据转换为我们需要的数据格式。 
1. 转换为`(用户，物品，评分)`格式
我们可以先人为设定一个行为的数据，比如点击`0.1` 分享`0.15`，评论`0.2`,收藏`0.25`,点赞`0.3` ,用户对一个文章只有这五种行为，如果一个用户对一篇文章发生了所有的行为，那分数加和就是`1`。这样通过具有一定规律的分数，我们就可以将行为转化为评分。这个在代码中会以UDF方式实现
2. 转化为`(用户，物品）`格式
如果我们不设定分数，我们可以直接选择一个行为比如`点击`，因为最终我们推荐文章给用户，目的就是让用户去点击的(当然如果你的目的是让用户收藏，你也可以选择收藏行为)。构成`（用户，点击物品, 1）` 这样的数据，只所有这里写上`1`,是为了将来计算相似度时使用，如果不明白，可以参看上一小节的图解去理解

我们代码中将采用转换为`(用户，物品，评分)`数据格式，采用余弦相似度进行计算
```





```xml
 <properties>
        <scala.version>2.11.12</scala.version>
        <play-json.version>2.3.9</play-json.version>
        <maven-scala-plugin.version>2.10.1</maven-scala-plugin.version>
        <scala-maven-plugin.version>3.2.0</scala-maven-plugin.version>
        <maven-assembly-plugin.version>2.6</maven-assembly-plugin.version>
        <spark.version>2.4.5</spark.version>
        <scope.type>compile</scope.type>
        <json.version>1.2.3</json.version>
        <hbase.version>1.3.6</hbase.version>
        <hadoop.version>2.7.6</hadoop.version>
        <!--compile provided-->
    </properties>

    <dependencies>

        <!--json 包-->
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>${json.version}</version>
        </dependency>


        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>${spark.version}</version>
            <scope>${scope.type}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>${spark.version}</version>
            <scope>${scope.type}</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.11</artifactId>
            <version>${spark.version}</version>
            <scope>${scope.type}</scope>
        </dependency>

        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.37</version>
        </dependency>
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
            <version>1.2.17</version>
            <scope>${scope.type}</scope>
        </dependency>
        <dependency>
            <groupId>commons-codec</groupId>
            <artifactId>commons-codec</artifactId>
            <version>1.6</version>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
            <scope>${scope.type}</scope>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-reflect</artifactId>
            <version>${scala.version}</version>
            <scope>${scope.type}</scope>
        </dependency>

        <dependency>
            <groupId>com.github.scopt</groupId>
            <artifactId>scopt_2.11</artifactId>
            <version>4.0.0-RC2</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-avro_2.11</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_2.11</artifactId>
            <version>${spark.version}</version>
            <scope>${scope.type}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-jdbc</artifactId>
            <version>2.3.7</version>
            <scope>${scope.type}</scope>
            <exclusions>
                <exclusion>
                    <groupId>javax.mail</groupId>
                    <artifactId>mail</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.eclipse.jetty.aggregate</groupId>
                    <artifactId>*</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
            <scope>${scope.type}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-server</artifactId>
            <version>${hbase.version}</version>
            <scope>${scope.type}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>${hbase.version}</version>
            <scope>${scope.type}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-hadoop2-compat</artifactId>
            <version>${hbase.version}</version>
            <scope>${scope.type}</scope>
        </dependency>
        <dependency>
            <groupId>org.jpmml</groupId>
            <artifactId>jpmml-sparkml</artifactId>
            <version>1.5.9</version>
        </dependency>
    </dependencies>
```



#### log4j.properties

```properties
# Set everything to be logged to the console
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Set the default spark-shell log level to WARN. When running the spark-shell, the
# log level for this class is used to overwrite the root logger's log level, so that
# the user can have different defaults for the shell and regular Spark apps.
log4j.logger.org.apache.spark.repl.Main=WARN

# Settings to quiet third party logs that are too verbose
log4j.logger.org.spark_project.jetty=WARN
log4j.logger.org.spark_project.jetty.com.qianfeng.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR

# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR

```

#### SparkHelper

```java
package com.qianfeng.recommend.util

import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

/**
  * @constructor 根据环境变量参数创建Spark Session ： dev \ test \ pro
  * @author QF
  * @date 2020/6/9 2:59 PM
  * @version V1.0
  */
object SparkHelper {

  /**
   * 根据传入的dev和应用名称--->获取spark不同平台下的sparksession
   * @param env
   * @param appName
   * @return
   */
  def getSparkSession(env: String, appName: String) = {
    env match {
        //返回生产级别的配置
      case "prod" => {
        //获取sparkconf对象
        val conf = new SparkConf()
          .setAppName(appName)
          .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
          //          .set("spark.sql.hive.metastore.version","2.3.0")
          .set("spark.sql.cbo.enabled", "true")
          .set("spark.hadoop.dfs.client.block.write.replace-datanode-on-failure.enable", "true")
          .set("spark.hadoop.dfs.client.block.write.replace-datanode-on-failure.policy", "NEVER")
          .set("spark.debug.maxToStringFields","200")

        //返回sparksession
        SparkSession
          .builder()
          .config(conf)
          .enableHiveSupport()
          .getOrCreate()
      }

        //返回测试环境的sparksession
      case "dev" => {
        val conf = new SparkConf()
          .setAppName(appName + " DEV")
          .setMaster("local[6]")
          .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
          //          .set("spark.sql.hive.metastore.version","1.2.1")
          .set("spark.sql.cbo.enabled", "true")
          .set("spark.hadoop.dfs.client.block.write.replace-datanode-on-failure.enable", "true")
          .set("spark.hadoop.dfs.client.block.write.replace-datanode-on-failure.policy", "NEVER")
            .set("spark.debug.maxToStringFields","200")
        //返回sparksession
        SparkSession
          .builder()
          .config(conf)
          .enableHiveSupport()
          .getOrCreate()
      }
        //其它情况
      case _ => {
        println("not match env, exits")
        System.exit(-1)
        null
      }
    }
  }

  /**
   * 测试
   * @param args
   */
  def main(args: Array[String]): Unit = {
    print(getSparkSession("dev","test-sparksession"))
  }
}

```



#### Config

```java
package com.qianfeng.recommend.config

import scopt.OptionParser

/**
 * 参数解析工具类
 *
 * @param env  运行时设置的环境 ，， dev/pro/test
 * @param checkpointDir   设置job运行时的检测点目录
 */
case class Config(
                   env: String = "",
                   checkpointDir: String ="",
                   zkHost:String = "hadoop01",
                   zkPort:String = "2181"
                 )


//config的对象
object Config {

  //解析参数列表
  def parseConfig(obj: Object, args: Array[String]): Config = {
    //正则\$ 替换成"";;;获取类名称
    val programName = obj.getClass.getSimpleName.replaceAll("\\$", "")
    //获取scopt对象
    val parser: OptionParser[Config] = new scopt.OptionParser[Config]("spark ss " + programName) {
      head(programName, "1.0") //应用程序版本，，可以不写
      //.required()是必须要传的参数  .action() :添加回调函数,传递值-将x(x是参数列表中对应的值)值copy给env  .text()： 添加该参数的使用描述
      opt[String]('e', "env").required().action((x, config) =>config.copy(env=x)).text("env: dev or prod")
      //使用程序名称进行匹配，然后赋予更多的参数
      programName match {
        case "ItemCF" =>
          opt[String]('h', "zkhost").required().action((x, config) =>config.copy(zkHost=x)).text("zkhost not must bu null")
          opt[String]('p', "zkport").required().action((x, config) =>config.copy(zkPort=x)).text("zkport not must bu null")


        case _ =>
      }
    }

    //解析
    val myConfig: Option[Config] = parser.parse(args, Config())

    //取对应的值
    myConfig match {
      case Some(conf) => conf
      case None => {
        // println("cannot parse args")
        System.exit(-1)
        null
      }
    }

  }
}

```

#### Hive-site.xml

```
直接拿上一个项目的文件即可
```

#### hdfs-site.xml

```
直接拿上一个项目的文件即可
```

#### yarn-site.xml

```
直接拿上一个项目的文件即可
```



#### HbaseUtil

```java
package com.qianfeng.recommend.hbase

import java.net.URI

import com.qianfeng.recommend.util.SparkHelper
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.hadoop.hbase.client.ConnectionFactory
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.{HFileOutputFormat2, LoadIncrementalHFiles, TableOutputFormat}
import org.apache.hadoop.hbase.{HBaseConfiguration, HConstants, KeyValue, TableName}
import org.apache.hadoop.mapreduce.Job
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{Row, SparkSession}
import org.slf4j.LoggerFactory
/**
  * @Description: 通过生成HFile的方式,将数据高效的写入HBASE
  * @Author: QF    
  * @Date: 2020/7/22 2:09 PM   
  * @Version V1.0 
  */
class HBaseUtil(spark: SparkSession,hbaseZK:String,hbaseZKPort:String) {

  private val log = LoggerFactory.getLogger("HBaseUtil")

  /**
   * Bulk load   桶装载---批次插入
    * 将hfile RDD加载到HBASE中
    * @param hfileRDD  hfile  RDD[(ImmutableBytesWritable,KeyValue)
    * @param tableName 加载到的hbase中的表名
    * @param hfileTmpPath hfile 临时存储目录
    */
  def loadHfileRDD2Hbase(hfileRDD:RDD[(ImmutableBytesWritable,KeyValue)],tableName: String,hfileTmpPath:String)={

    val hfilePath = hfileTmpPath +"/"+ String.valueOf(System.currentTimeMillis())
    val hbaseConf = setHBaseConfig() //设置zk的ip地址和端口
    hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, tableName) //设置输出表名为
    val conn = ConnectionFactory.createConnection(hbaseConf)  //获取hbase的连接
    log.warn("create hbase connection: "+conn.toString)
    val admin = conn.getAdmin  //获取admin操作对象
    val table = conn.getTable(TableName.valueOf(tableName))  //获取表操作对象
    val job = Job.getInstance(hbaseConf)  //获取一个job实例
    //设置job的输出格式
    job.setMapOutputKeyClass(classOf[ImmutableBytesWritable])  //设置map阶段输出key的类型
    job.setMapOutputValueClass(classOf[KeyValue])   //设置map阶段输出Value的类型
    job.setOutputFormatClass(classOf[HFileOutputFormat2])  //设置输出格式类
    HFileOutputFormat2.configureIncrementalLoad(job, table, conn.getRegionLocator(TableName.valueOf(tableName)))
    // 如果路径存在就删除，因为加了毫秒作为目录，因此一般不会存在
    deleteFileExist(hfilePath)

    hfileRDD.coalesce(10).saveAsNewAPIHadoopFile(hfilePath, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat2], job.getConfiguration)
    val bulkLoader = new LoadIncrementalHFiles(hbaseConf)
    // load 正常执行完毕后， 在hdfs生成的hfile文件就被mv走，不存在了
    bulkLoader.doBulkLoad(new Path(hfilePath), admin, table, conn.getRegionLocator(TableName.valueOf(tableName)))
    log.warn("load hfile to hbase success! hfile path: "+hfilePath)

  }


  /**
    * 配置HBASE参数
    * @return
    */
  def setHBaseConfig():Configuration={
    val hbaseConf:Configuration = HBaseConfiguration.create()
    hbaseConf.set(HConstants.ZOOKEEPER_QUORUM, hbaseZK)  //conf.set("hbase.zookeeper.quorum","hadoopp01")
    hbaseConf.set(HConstants.ZOOKEEPER_CLIENT_PORT, hbaseZKPort) //hbase.zookeeper.property.clientPort
    hbaseConf
  }

  /**
    * 如果hdfs存在传入的文件路径，则删除
    * @param filePath  文件路径
    */
  def deleteFileExist(filePath: String): Unit ={
    val output = new Path(filePath)
    val hdfs = FileSystem.get(new URI(filePath), new Configuration)
    if (hdfs.exists(output)){
      hdfs.delete(output, true)  //true : 递归
    }
  }
}


object HBaseUtil {
  def apply(spark: SparkSession,hbaseZK:String,hbaseZKPort:String): HBaseUtil = new HBaseUtil(spark,hbaseZK,hbaseZKPort)

  def main(args: Array[String]): Unit = {
    HBaseUtil(SparkHelper.getSparkSession("dev","test"),"hadoop01","2181") //.loadHfileRDD2Hbase()  //.deleteFileExist("/words")
  }
}
```

#### Action

```java
package com.qianfeng.recommend.constant

/**
 * 事件枚举
 */
object Action extends Enumeration {
  type Action = Value

  val Click = Value("点击")  //0.1分
  val Share = Value("分享")   //0.15分
  val Comment = Value("评论")
  val Collect = Value("收藏")
  val Like = Value("点赞")

  def showAll = this.values.foreach(println)

  def withNameOpt(s: String): Option[Value] = values.find(_.toString == s)
}

```

#### Constant

```java
package com.qianfeng.recommend.constant

/**
  * @Description: 定义常量
  * @Author: QF    
  * @Date: 2020/7/16 10:01 PM   
  * @Version V1.0 
  */
object Constant {

  // 新闻文章的时效时间，当前前按照15天计算，表示文章在15天内具有更大的阅读价值，且时间越远价值越低
  // 如果实在不理解这个没有关系，可以不加入时间衰减因素
  val ARTICLE_AGING_TIME = 14
}

```

#### DateUtil

```java
package com.qianfeng.recommend.util;

import java.text.DateFormat;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.Date;
import java.util.List;

//时间工具类转换
public class DateUtil {

    private static final String date_format = "yyyyMMdd";
    private static ThreadLocal<DateFormat> threadLocal = new ThreadLocal<DateFormat>();

    public static DateFormat getDateFormat()
    {
        DateFormat df = threadLocal.get();
        if(df==null){
            df = new SimpleDateFormat(date_format);
            threadLocal.set(df);
        }
        return df;
    }


    /**
     * 判断传入时间距离现在的天数
     * @param dataStr
     * @return
     * @throws ParseException
     */
    public static int diffDayToNow(String dataStr) throws ParseException {

        Calendar cal = Calendar.getInstance();
        cal.setTime(getDateFormat().parse(dataStr));
        long time1 = cal.getTimeInMillis();
        cal.setTime(new Date());
        long time2 = cal.getTimeInMillis();
        long between_days = (time2 - time1) / (1000 * 3600 * 24);

        return Integer.parseInt(String.valueOf(between_days));
    }

    /**
     * 获取指定范围内的所有日期字符串，包括开始和结束
     * @param startDateStr
     * @param endDateStr
     * @return
     * @throws ParseException
     */
    public static List<String> getRangeDateStr(String startDateStr, String endDateStr) throws ParseException {
        List<String> dateStrList=new ArrayList<String>();
        dateStrList.add(startDateStr);
        Calendar calStart=Calendar.getInstance();
        calStart.setTime(getDateFormat().parse(startDateStr));
        Calendar calEnd=Calendar.getInstance();
        calEnd.setTime(getDateFormat().parse(endDateStr));
        while (calEnd.after(calStart)){
            calStart.add(Calendar.DAY_OF_MONTH,1);
            dateStrList.add(getDateFormat().format(calStart.getTime()));
        }
        return dateStrList;
    }

    //测试
    public static void main(String[] args) throws ParseException {
        System.out.println(diffDayToNow("20201020"));
    }
}

```



# day01作业

1、协同过滤的电影推荐



#### 创建hbase存储表

```properties
# 因为我们会将最终的结果存入到HBASE中，我们先来创建一个HBASE表
# 进入到HBASE SHELL 执行如下命令
# 创建namespace
create_namespace 'recommend'
# 创建 news-cf 表，存放协同过滤算法的推荐结果数据
create 'recommend:news-cf','f1'

# 我们代码中会将itemcf生成的推荐结果都存入到该表中，在列族 f1 下，itemcf结果的列名为itemcf
```

#### RatingUDF

```java
package com.qianfeng.recommend.udfs

import java.text.ParseException

import com.qianfeng.recommend.constant.{Action, Constant}
import com.qianfeng.recommend.util.DateUtil


/**
  * @Description: 给用户行为打分的UDF，考虑时间衰减
  * @Author: QF    
  * @Date: 2020/7/16 9:45 PM   
  * @Version V1.0 
  */
object RatingUDF {

  /**
    * 通过用户行为，和该行为发生的时间距离当前时间的长短，赋予行为一个评分
    * @param action
    * @param date
    */
  def action2rating(action:String,date:String):Float={
    // 行为权重 * 时间权重
    val rating = getActionWeight(action) * getSigmoidTimeWeight(date)
    rating
  }


  /**
    * 定义一个行为权重函数，对每一个行为按照重要程度赋值一个权重
    * @param action
    * @return
    */
  def getActionWeight(action:String ): Float={
    Action.withNameOpt(action).getOrElse() match  {
      case Action.Click=>  0.1f
      case Action.Share=> 0.15f
      case Action.Comment=> 0.2f
      case Action.Collect => 0.25f
      case Action.Like => 0.3f
      case _ =>  0.0f
    }
  }


  /**
    * 注： 如果实在不理解这个没有关系，可以不加入时间衰减因素
    * 时间衰减函数，表示了一个行为发生的时间距离当前时间越远，那这个行为的权重就应该较低，相当于给行为加上了时间权重
    * Sigmoid 归一化时间权重，将权重值归一到 0~1 之间
    * Sigmoid = 1 /(1 + Math.exp(1.0-x));
    * w = Sigmoid((AGING_TIME-x-7)*0.8) 条件；[AGING_TIME-x<0 时,AGING_TIME-x=1] 约值域：(0.03,0.98)
    * AGING_TIME=14 如果调整AGING_TIME常量值，需要同时调整权重函数
    */
  def getSigmoidTimeWeight(date: String): Float = {
    try { // 获取时间隔
      var interval = Constant.ARTICLE_AGING_TIME - DateUtil.diffDayToNow(date)
      if (interval < 0) interval = 1
      val x = interval.toDouble - 7
      return Sigmoid(x * 0.8).toFloat
    } catch {
      case e: ParseException =>
        e.printStackTrace()
    }
    0.0f
  }

  /**
    * Sigmoid 归一化函数
    *
    * @param x
    * @return
    */
  private def Sigmoid(x: Double) = 1 / (1 + Math.exp(1.0 - x))


  /*
  6 距离6天  衰减值=1/331=0.0001
  1 距离1天  衰减值=1/1.0022
   */
  def main(args: Array[String]): Unit = {
    println((1 + Math.exp(1.0 - (4.8))))
  }
}
```

#### ModelData

```java
package com.qianfeng.recommend.transform

import com.qianfeng.recommend.udfs.RatingUDF
import org.apache.spark.sql
import org.apache.spark.sql.SparkSession

/**
  * @Description: 生成评分数据
  * @Author: QF    
  * @Date: 2020/7/21 10:47 AM   
  * @Version V1.0 
  */
class ModelData(spark: SparkSession, env:String) {

  //注册udf函数  (函数名称，函数)
  spark.udf.register("action2rating",RatingUDF.action2rating _)

  var limitData = ""
  if (env.equalsIgnoreCase("dev")) {
    limitData = " limit 1000"
  }

  /**
    * 从数仓dwb_news.user_article_action读取原始用户文章行为数据, 数据格式如下
    * 51917125 | 9049382 | 点击   | 20200715
    * 51876174 | 9049074 | 点击   | 20200715
    * 51960095 | 9049573 | 收藏   | 20200715
    *
    * @return
    */
  def loadSourceUserArticleActionData(): sql.DataFrame = {

    val loadSourceSql =
      s"""
         |select uid,aid,action,action_date from dwb_news.user_article_action
      """.stripMargin
    //执行sql
    spark.sql(loadSourceSql)
  }

  /**
    * 将用户对每个文章的行为转化为评分，评分的计算由action2rating udf完成
    */
  def genRatingToEachAction()={
    //将用户文章行为数据存储到临时表中，表名称为source_data
    loadSourceUserArticleActionData().createOrReplaceTempView("source_data")
    // user action to rating data
    val a2rSQL=
      """
        |select uid,aid,action,action_date,action2rating(action,action_date)as rating from source_data
      """.stripMargin
    //评分数据存储到source_data_rating临时表中
    spark.sql(a2rSQL).createOrReplaceTempView("source_data_rating")
  }

  /**
    * 一个用户对一个文章所有行为的评分求和，得到用户，文章，评分 数据表
    * +--------+-------+-------------------+
    * |uid     |aid    |rating             |
    * +--------+-------+-------------------+
    * |52041126|9048928|0.28577226400375366|
    *
    * @return
    */
  def genUserRatingData():sql.DataFrame={
    //为每一个行为生成评分
    genRatingToEachAction()

    val ratingSQL=
      """
        |select
        |cast(uid as bigint),
        |cast(aid as bigint),
        |cast(sum(rating) as double) as rating
        |from source_data_rating
        |group by uid,aid
        |order by uid desc
      """.stripMargin
    spark.sql(ratingSQL)
  }



  /**
    * 从数仓dwb_news.article_base_info 读取抽取的文章基础信息数据， 数据格式如下
          article_id | article_num | img_num | type_name | pub_gap
      ------------+-------------+---------+-----------+---------
       73789      |        2178 |      18 | 娱乐      |       6
       73809      |         348 |       4 | 娱乐      |       5
       73355      |        1297 |       3 | 情感      |       5

    *
    * @return
    */
  def loadSourceArticleBaseInfoData(): sql.DataFrame = {

    val loadSourceSql =
      s"""
         select article_id,cast(article_num as int) ,cast(img_num as int),type_name,cast(pub_gap as int)
         from dwb_news.article_base_info
      """.stripMargin

    spark.sql(loadSourceSql)
  }
  
  /**
   * 从数仓dwb_news.user_base_feature 表中读取用户基本特征
   *
   * uid   | gender | age | email_suffix
   * --------+--------+-----+--------------
   * 101723 | 男     | 30  | 139.net
   * 101724 | 男     | 21  | 139.net
   * 101727 | 女     | 49  | sdu.edu.cn
   * 101725 | 男     | 49  | msn.com
   * 101726 | 女     | 72  | 139.net
   *
   * @return
   */
  def loadSourceUserBaseFeature(): sql.DataFrame = {

    val loadSourceSql =
      s"""
         |select
         |uid,
         |gender,
         |cast(age as int),
         |email_suffix
         |from dwb_news.user_base_feature
      """.stripMargin

    spark.sql(loadSourceSql)
  }

  /**
    * 生成简单测试数据，(用户,物品,评分), 当没有真实数据时，可以用如下数据作为算法输入做简单算法流程测试
    * @return
    */
  def someTestData():sql.DataFrame={
    val array = Seq(
      (3,107,5.0),
      (2,102,2.5),
      (5,106,4.0),
      (1,101,5.0),
      (1,102,3.0),
      (5,102,3.0),
      (3,101,2.5),
      (5,104,4.0),
      (1,103,2.5),
      (2,103,5.0),
      (5,105,3.5),
      (4,104,4.5),
      (5,103,2.0),
      (2,101,2.0),
      (4,101,5.0),
      (3,105,4.5),
      (4,103,3.0),
      (5,101,4.0),
      (4,106,4.0),
      (2,104,2.0),
      (3,104,4.0)).map(x=>(x._1.toLong,x._2.toLong,x._3.toDouble))

    spark.createDataFrame(array).toDF("uid","aid","rating")
  }
}

```

#### ItemCFModelData

```java
package com.qianfeng.recommend.transform

import org.apache.hadoop.hbase.KeyValue
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql
import org.apache.spark.sql.{DataFrame, Row, SparkSession}

import scala.collection.mutable.ListBuffer

/**
  * @Description: 为ItemCF模型训练提供数据处理方法
  * @Author: QF    
  * @Date: 2020/7/16 9:27 PM   
  * @Version V1.0 
  */
class ItemCFModelData(spark: SparkSession, env:String) extends ModelData(spark: SparkSession, env:String) {

  /**
    * 将用户评分表转换为分布式矩阵表示 输入source_data_rating 这个表
    * @param df
    * @return
    */
  def ratingDF2Matrix(df:sql.DataFrame): CoordinateMatrix={
    val matrixRDD: RDD[MatrixEntry] = df.rdd.map {
      //模式匹配
      case Row(uid: Long, aid: Long, rating: Double) =>
        MatrixEntry(uid.toLong, aid.toLong, rating.toDouble) //返回MatrixEntry对象
        /*
        119 3 3.0
        120 4 5.2
         */
    }
    //构造返回坐标举证
    val cm = new CoordinateMatrix(matrixRDD)
    /*
    ([MatrixEntry(119 3 3.0),MatrixEntry120 4 5.2,],0,0)
     */
    cm
  }

  /**
    * 相似度矩阵转换为DataFrame
    * @param coordinateMatrix 由IndexedRowMatrix 的columnSimilarities，计算得到相似度坐标矩阵
    * @return
    */
  def similarityMatrix2DF(coordinateMatrix: CoordinateMatrix):sql.DataFrame={
    // 矩阵变换为String RDD
    val transformedRDD: RDD[(String, String, Double)] = coordinateMatrix.entries.map {
      case MatrixEntry(row: Long, col: Long, sim: Double) =>
        (row.toString, col.toString, sim)
    }
    // 经过columnSimilarity 计算得到的相似度矩阵对称矩阵，
    // 它只保留了是上三角矩阵，同时这个上三角矩阵去掉了物品A与其自身相似度(总是1)物品对和物品之间相似度是0的物品对
    // 我们矩阵转为DataFrame，将物品对补全
    val simDF =  spark.createDataFrame(transformedRDD).toDF("aid","sim_aid","sim")
    //连接每一个的分区的文章和文章的相似度
    simDF.union(simDF.select("sim_aid","aid","sim"))
  }


  /**
    * 关联评分DF和物品相似度DF，并将用户对物品的评分点乘关联到物品的相似度，得到用户对单个物品相似物品的评分
    * 类似结果如下
    * +--------+-------+-------------------+-------+-------+------------------+-------------------+
    * |uid     |aid    |rating             |aid    |sim_aid|sim               |rsp = (rating * sim)             |
    * +--------+-------+-------------------+-------+-------+------------------+-------------------+
    * |51876453|9015927|0.09002494812011719|9015927|9049026|0.7453559924999298|0.06710063455582463|
    * |51814918|9049258|0.09002494812011719|9049258|9049597|0.9284766908852593|0.08358606592768356|
    * |51924704|9049258|0.22506237030029297|9049258|9049597|0.9284766908852593|0.20896516481920888|
    * |52015583|9048856|0.22506237030029297|9048856|9049649|1.0               |0.22506237030029297|
    * |51951576|9048965|0.22506237030029297|9048965|9048968|0.4472135954999579|0.10065095183373697|
    * +--------+-------+-------------------+-------+-------+------------------+-------------------+
    *
    * @param ratingDF
    * @param simDF
    * @return
    */
  def joinRatingAndSimilarity(ratingDF: sql.DataFrame,simDF:sql.DataFrame):sql.DataFrame={
    //用户行为总评分的df   uid aid ratings
    ratingDF.createOrReplaceTempView("user_rating")
    //文章和文章的相似度  aid sim_aid 相似度
    simDF.createOrReplaceTempView("sim_item")
    val joinSQL=
      """
        |select
        |t1.uid,
        |t1.aid,
        |t1.rating,
        |t2.aid as aid_x,
        |t2.sim_aid,
        |t2.sim,
        |t1.rating*t2.sim as rsp
        |from user_rating as t1
        |left join sim_item as t2
        |on t1.aid = t2.aid
        |where t2.sim is not null
      """.stripMargin
    spark.sql(joinSQL)

  }


  /**
    * 为用户推荐top-k的内容，同时会过滤掉用户已经有过行为的内容
    * @param ratingSimDF 评分DF和相似DF JOIN
    * @param topK
    * @return
    */
  def recommendForAllUser(ratingSimDF :sql.DataFrame,topK:Int=25): sql.DataFrame={
    //用户 文章 相似文章id 相似度 得分*相似度 相似度的数据
    ratingSimDF.createOrReplaceTempView("rating_sim")

    val prefSQL =
      s"""
        |with t1 as
        |(
        |select uid,sim_aid,sum(rsp)/sum(sim) as pred_rating
        |from rating_sim
        |group by uid,sim_aid
        |),
        |
        |t2 as (
        |select
        |t1.*
        |from t1
        |left join user_rating as ur
        |on t1.uid=ur.uid and t1.sim_aid=ur.aid
        |where ur.rating is null
        |),
        |
        |t3 as (
        | select
        | uid,sim_aid , pred_rating,
        | ROW_NUMBER() OVER (PARTITION BY uid order by pred_rating desc ) as num
        | from t2
        |) select
        |cast(uid as int ) as uid,
        |cast(sim_aid as int) as sim_aid,
        |cast(pred_rating as double) as pred_rating
        |from t3 where num <= $topK
        |
      """.stripMargin
    //执行sql语句
    val prefDF =  spark.sql(prefSQL)
    prefDF
//    import spark.implicits._
//    // (uid,sim_aid,pred_rating) 转换为 [uid,[(sim_aid,pred_rating),(sim_aid,pred_rating),...]] 作为推荐结果 并根据预测评分排序
//    prefDF.rdd
//      .map(row => (row.getInt(0), (row.getInt(1), row.getDouble(2))))
//      .groupByKey()
//      .mapValues(sr => {
//        var sequence = Seq[(Int , Double)]()
//        sr.foreach(x=>{
//            sequence :+= (x._1, x._2)
//        })
//        sequence.sortBy(-_._2)
//      }).toDF("uid", "recommendations")
  }


  /**
    * 推荐的结果按用户ID分组，合并为一列
    * (uid,sim_aid,pred_rating) 转换为 [uid,[(sim_aid,pred_rating),(sim_aid,pred_rating),...]] 作为推荐结果 并根据预测评分排序
    * 最终返回DF结果类似如下
    * +-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    *  |uid  |recommended                                                                                                                                                                                                                                                                             |
    *  +-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    *  |51554|[[402, 0.3334009498357773], [200, 0.28577226400375366], [201, 0.28577226400375366], [195, 0.28577226400375366], [194, 0.28577226400375366], [202, 0.28577226400375366], [205, 0.2150895099015218], [262, 0.21475638339549794], [287, 0.20036066829231725], [281, 0.19467683094823043]]  |
    *  |51392|[[399, 0.28577226400375366], [402, 0.28577226400375366], [392, 0.2585274432595102], [385, 0.22768240869796444], [400, 0.22595028227749064], [342, 0.22110066923687446], [290, 0.21824826246055046], [292, 0.21410174015282366], [315, 0.2125652485650218], [289, 0.21254394031515117]]  |
    *  |51716|[[398, 0.3097893748819769], [241, 0.2795277971520104], [360, 0.26616003172023667], [293, 0.2640595963392586], [237, 0.26229175733792875], [382, 0.2595780625641913], [259, 0.25900229659766044], [280, 0.257957302723468], [233, 0.25741724160888974], [230, 0.25635324586719577]]      |
    *  |51717|[[374, 0.23492200852534456], [379, 0.2330988076172057], [348, 0.23255879571410426], [376, 0.23188501571896075], [354, 0.23092542423741497], [358, 0.22814669274760022], [364, 0.22771463292827812], [362, 0.2271160199054104], [368, 0.2245965690097818], [381, 0.22033059154721482]]   |
    *
    *
    * @param recoDF
    * @return
    */
  def recommendDataConvert(recoDF:DataFrame): sql.DataFrame={
    import spark.implicits._
    // (uid,sim_aid,pred_rating) 转换为 [uid,[(sim_aid,pred_rating),(sim_aid,pred_rating),...]] 作为推荐结果 并根据预测评分排序
    recoDF.rdd
      .map(row => (row.getInt(0), (row.getInt(1), row.getDouble(2))))
      .groupByKey()
      .mapValues(sr => {
        var sequence = Seq[(Int , Double)]()  //初始化序列
        //迭代循环
        sr.foreach(x=>{
          sequence :+= (x._1, x._2)
        })
        //根据预测得分  -倒序排序
        sequence.sortBy(-_._2)
      }).toDF("uid", "recommendations")
  }

  /**
    * 推荐算法ItemCF 产生的结果生成HFile RDD
    * 需要添加建立好hbase表，行键将会存入uid, 列族为 f1 , 列为 itemcf ,值为推荐的物品和评分
    * 创建hbase表样例如：
    *    create_namespace 'recommend'
    *    create 'recommend:news-cf','f1'
    * @param itemCFDF itemCF 产生解推荐结果,格式 [uid,[[aid,rating],[aid,rating],...]]]
    * @return
    */
  def itemCFDF2HFile(itemCFDF:sql.DataFrame):RDD[(ImmutableBytesWritable,KeyValue)]={

    val hfileRdd = itemCFDF.rdd
      .sortBy(x =>x.get(0).toString)
      .flatMap(row=>{
        val uid = row.getInt(0).toString
        val items = row.getAs[Seq[Row]](1)
          .map(item=>{item.getInt(0).toString+":"+item.getDouble(1).formatted("%.4f")})
          .mkString(",")
        //初始化一个容器
        val listBuffer = new ListBuffer[(ImmutableBytesWritable, KeyValue)]
        val kv1: KeyValue = new KeyValue(Bytes.toBytes(uid), Bytes.toBytes("f1"), Bytes.toBytes("itemcf"), Bytes.toBytes(items))
        // 多个列按列名字典顺序append
        listBuffer.append((new ImmutableBytesWritable, kv1))
        listBuffer
      })
    hfileRdd
  }

  def predictForTestData(ratingSimDF :sql.DataFrame,testDF:sql.DataFrame): sql.DataFrame={
    //用户文章、相似文章、相似度、相似度乘以得分
    ratingSimDF.createOrReplaceTempView("rating_sim")
    //测试数据  uid aid 得分
    testDF.createOrReplaceTempView("test_data")

    val predictSQL =
      s"""
         |with t1 as
         |(
         |select uid,sim_aid,sum(rsp)/sum(sim) as pred_rating
         |from rating_sim
         |group by uid,sim_aid
         |),
         |
         |t2 as (
         |select * from test_data
         |)
         |select
         |t2.*,
         |t1.pred_rating
         |from t2
         |inner join t1
         |on t2.uid=t1.uid and t2.aid=t1.sim_aid
         |where t1.pred_rating is not null
      """.stripMargin

    val predictDF =  spark.sql(predictSQL)
    predictDF
  }


}

object ItemCFModelData {
  def apply(spark: SparkSession, env: String): ItemCFModelData = new ItemCFModelData(spark, env)
}
```

#### ItemCF

```java
package com.qianfeng.recommend

import com.qianfeng.recommend.config.Config
import com.qianfeng.recommend.hbase.HBaseUtil
import com.qianfeng.recommend.transform.ItemCFModelData
import com.qianfeng.recommend.util.SparkHelper
import org.apache.hadoop.hbase.KeyValue
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.log4j.{Level, Logger}
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.mllib.linalg.distributed.CoordinateMatrix
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, SaveMode}
import org.slf4j.LoggerFactory

/**
 * 基于物品的相似度进行推荐
 * 1、命令行参数解析
 * 2、使用spark查询hive中的dwb_news.user_article_action，并将该表中的action转换为评分
 * 3、将第二步中的DF转换成矩阵
 * 4、求矩阵中物品的相似度
 *
 */
object ItemCF {
  //日志打印对象
  val log = LoggerFactory.getLogger(ItemCF.getClass)
  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.WARN)
    System.setProperty("HADOOP_USER_NAME", "root")

    // 解析命令行参数
    val params = Config.parseConfig(ItemCF, args)
    log.warn("job running please wait ... ")
    // init spark session
    val ss = SparkHelper.getSparkSession(params.env, "item-cf")

    //查询hive中用户物品行为数据：dwb_news.user_article_action
    // 基础数据处理工具类
    val modelData = ItemCFModelData(ss, params.env)

    //查询dwb_news.user_article_action
    //val uaaDF: DataFrame = modelData.loadSourceUserArticleActionData()

    //将用户文章行为数据生成总评分
    val uaDF: DataFrame = modelData.genUserRatingData()


    //将评分数据随机划分训练数据和测试数据  ---训练数据80%  测试数据20%
    val Array(training, test) = uaDF.randomSplit(Array(0.8, 0.2))
    training.cache()  //将训练数据缓存
    //training.orderBy("uid")show(false)
    // 没有真实数据时，可以加载一些简单测试数据，运行算法流程
    //val ratingDF = modelData.someTestData().randomSplit(Array(0.8,0.2))

    //用户行为评分DataFrame 转换为分布式坐标矩阵
    val ratingMatrix:CoordinateMatrix = modelData.ratingDF2Matrix(training)

    // 分布式坐标矩阵转换为行索引矩阵，计算行索引矩阵各个列之间的相似度，得到相似度矩阵
    val similarityMartix=  ratingMatrix.toRowMatrix().columnSimilarities()
    println(similarityMartix)  //相似矩阵

    // 相似度矩阵转换为DataFrame
    val simDF = modelData.similarityMatrix2DF(similarityMartix)
    simDF.sort("aid","sim_aid").show(100,false)

    // 评分DataFrame和相似度DataFrame关联，同时计算评分和相似度的乘积
    val joinDF = modelData.joinRatingAndSimilarity(training,simDF)
    //joinDF.sort("uid","aid_x")show(100,false)
    joinDF.cache()
    joinDF.show(false)
    //使用测试数据进行预测
    val predictDF = modelData.predictForTestData(joinDF,test)
    predictDF.show(false)
    // 评估器，这里用来计算预测出来的评分和原始测试集的评分的均方误差，来衡量训练出的模型优劣
    val evaluator = new RegressionEvaluator()
      .setLabelCol("rating")  //真实的评分列
      .setPredictionCol("pred_rating")  //预测评分列
    // 计算均方误差
    val rmse = evaluator.setMetricName("rmse").evaluate(predictDF)
    log.warn(s"[ItemCF算法] 均方根误差(Root-mean-square error) = $rmse")

    //为所有用户推荐topK的内容
    val recoDF = modelData.recommendForAllUser(joinDF,params.topK)
    recoDF.show(false)
    // 推荐结果保存一份到到HDFS
    recoDF.write.mode(SaveMode.Overwrite).format("ORC").saveAsTable("dwb_news.itemcf")

    //将结果存储到hbase中
    val convertDF: DataFrame = modelData.recommendDataConvert(recoDF)

    //将df转换成RDD
    log.warn("start gen hfile, load itemcf result to hbase ! ")
    // 将推荐结果的DataFrame转换为HFile RDD
    val hfileRDD:RDD[(ImmutableBytesWritable,KeyValue)] = modelData.itemCFDF2HFile(convertDF)

    //获取hbase的工具
    val hBaseUtil =  HBaseUtil(ss,params.zkHost,params.zkPort)
    hBaseUtil.loadHfileRDD2Hbase(hfileRDD,"recommend:news-cf","/recommend/itemcf")

    ss.stop()
  }
}

```





本地测试：

```properties
-e dev -h hadoop01 -p 2181 -f /recommend/itemcf -k 100 -t recommend:news-cf
```



#### pom.xml

```xml
<build>
        <sourceDirectory>src/main/scala</sourceDirectory>
        <testSourceDirectory>src/test/scala</testSourceDirectory>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>${maven-assembly-plugin.version}</version>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>${scala-maven-plugin.version}</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                        <configuration>
                            <args>
                                <arg>-dependencyfile</arg>
                                <arg>${project.build.directory}/.scala_dependencies</arg>
                            </args>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-archetype-plugin</artifactId>
                <version>2.2</version>
            </plugin>

            <plugin>
                <groupId>org.codehaus.mojo</groupId>
                <artifactId>build-helper-maven-plugin</artifactId>
                <version>1.8</version>
                <executions>
                    <!-- Add src/main/scala to eclipse build path -->
                    <execution>
                        <id>add-source</id>
                        <phase>generate-sources</phase>
                        <goals>
                            <goal>add-source</goal>
                        </goals>
                        <configuration>
                            <sources>
                                <source>src/main/java</source>
                            </sources>
                        </configuration>
                    </execution>
                    <!-- Add src/test/scala to eclipse build path -->
                    <execution>
                        <id>add-test-source</id>
                        <phase>generate-test-sources</phase>
                        <goals>
                            <goal>add-test-source</goal>
                        </goals>
                        <configuration>
                            <sources>
                                <source>src/test/java</source>
                            </sources>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
```



线上测试：

```shell
# 编译打包
mvn clean package -DskipTests -Dscope.type=provided 
# 将编译好的JAR,recommend-1.0.0-jar-with-dependencies.jar,下载到你的服务器上
# 我放置到了 /opt/app/recommend/ 目录下
mkdir -p /opt/app/recommend/
# 同时也提供了一个编译好JAR，大家最好自己编译
wget -P  /opt/app/recommend/  http://doc.yihongyeyan.com/qf/project/soft/app/recommend-1.0.0-jar-with-dependencies.jar
# 之后执行如下命令，提交我们测程序到Yarn上，这次我们使用client方式提交作业，还记得之前数据落地程序吗，我们是采用cluster方式， 注意作业参数zk地址替换为你自己的地址
${SPARK_HOME}/bin/spark-submit \
--master yarn \
--deploy-mode client \
--conf spark.executor.heartbeatInterval=120s \
--conf spark.network.timeout=600s \
--conf spark.sql.catalogImplementation=hive \
--conf spark.sql.shuffle.partitions=20 \
--conf spark.yarn.submit.waitAppCompletion=true \
--name item-cf \
--driver-memory 512M \
--num-executors 1 \
--executor-cores 1 \
--executor-memory 1G \
--class com.qf.bigdata.recommend.ItemCF \
/opt/app/recommend/recommend-1.0.0-jar-with-dependencies.jar \
-e prod -h hadoop01 -p 2181 -f /recommend/itemcf -k 100 -t recommend:news-cf
```





## 2.4 ALS召回模型

ALS基于模型的协同过滤算法，最小交替二乘法。

### ALS演示示例

![1603872759721](%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/1603872759721.png)





### ALS推荐实现步骤

1. 从`dwb_news.user_article_action `加载我们生成原始数据，然后将行为转换为评分，这个和`ItemCF`一致，代码可以复用
2. 使用`Spark MLlib 提供的ALS`算法，获取ALS的算法模型，并未模型设置参数，求解出用户向量矩阵和物品向量矩阵。
3. 评估模型优劣，这个也和`ItemCF`一致，我们依然使用同样的方法计算`RMSE`
4. 给所有用户生产推荐列表，过滤掉用户已经有过行为的物品，取topK
5. 将推荐结果存储到`HBASE`中，这个同样和`ItemCF` 一致，我们同样存储到`HBASE`中`recommend:news-cf`中，列族依然是`f1`,  只不过我们将其结果存储到一个新的列`als` 列中。



#### ALSModelData

```java
package com.qianfeng.recommend.transform

import org.apache.hadoop.hbase.KeyValue
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.rdd.RDD
import org.apache.spark.sql
import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.sql.functions.explode

import scala.collection.mutable.ListBuffer

/**
  * @Description:
  * @Author: QF    
  * @Date: 2020/7/21 10:08 AM   
  * @Version V1.0 
  */
class ALSModelData(spark: SparkSession,env:String)  extends ModelData(spark: SparkSession,env:String) {

  import spark.implicits._

  /**
    * 从ALS推荐的结果中过滤掉用户已经产生过行为的物品
    * @param ratingDF 原始用户评分DF
    * @param alsDF als 给用户推荐的结果
    * @return
    */
  def filterALSRecommendForAllUser(ratingDF: sql.DataFrame,alsDF:sql.DataFrame):sql.DataFrame={

    // 将recommendations 数组转为多行 123 [(788,3.0),(125,2.9)]
    val transDF =  alsDF.withColumn("recommendations",explode($"recommendations"))
      .withColumn("pred_aid",$"recommendations.aid")
      .withColumn("pred_rating",$"recommendations.rating")

    //用户文章行为评分数据集  uid aid rating
    ratingDF.createOrReplaceTempView("user_rating")
    //用户推荐文章和评分数据集 uid pre_aid pre_rating
    transDF.createOrReplaceTempView("als_pred")

    // 过滤掉用户已经有行为的物品
    val filterSQL=
      """
        |select
        |cast(t1.uid as int) as uid,
        |cast(t1.pred_aid as int) as pred_aid,
        |cast(t1.pred_rating as double) as pred_rating
        |from als_pred as t1
        |left join user_rating as t2
        |on t1.pred_aid = t2.aid and t1.uid=t2.uid
        |where t2.rating is  null
      """.stripMargin
    val filterDF = spark.sql(filterSQL)
    filterDF

    // 转换成推荐列表格式
    // (uid,sim_aid,pred_rating) 转换为 [uid,(sim_aid,pred_rating),(sim_aid,pred_rating),...] 作为推荐结果 并根据预测评分排序
//    filterDF.rdd
//      .map(row => (row.getInt(0), (row.getInt(1), row.getDouble(2))))
//      .groupByKey()
//      .mapValues(sr => {
//        var sequence = Seq[(Int , Double)]()
//        sr.foreach(x=>{
//          sequence :+= (x._1, x._2)
//        })
//        sequence
//      }).toDF("uid", "recommendations")


  }
  /**
    * 推荐的结果按用户ID分组，合并为一列
    * (uid,sim_aid,pred_rating) 转换为 [uid,[(sim_aid,pred_rating),(sim_aid,pred_rating),...]] 作为推荐结果 并根据预测评分排序
    * @param recoDF
    * @return
    */
  def recommendDataConvert(recoDF:DataFrame): sql.DataFrame={
    import spark.implicits._
    // (uid,sim_aid,pred_rating) 转换为 [uid,[(sim_aid,pred_rating),(sim_aid,pred_rating),...]] 作为推荐结果 并根据预测评分排序
    recoDF.rdd
      .map(row => (row.getInt(0), (row.getInt(1), row.getDouble(2))))
      .groupByKey()
      .mapValues(sr => {
        var sequence = Seq[(Int , Double)]()
        sr.foreach(x=>{
          sequence :+= (x._1, x._2)
        })
        sequence.sortBy(-_._2)
      }).toDF("uid", "recommendations")
  }

  /**
    * 推荐算法ALS 产生的结果生成HFile RDD
    * 需要添加建立好hbase表，行键将会存入uid, 列族为 f1 , 列为 als ,值为推荐的物品和评分
    * 创建hbase表样例如：
    *   create_namespace 'recommend'
    *   create 'recommend:news-cf','f1'
    * @param alsDF als产生解推荐结果,格式 [uid,[[aid,rating],[aid,rating],...]]]
    * @return
    */
  def alsDF2HFile(alsDF:sql.DataFrame):RDD[(ImmutableBytesWritable,KeyValue)]={

    val hfileRdd = alsDF.rdd.sortBy(x =>x.get(0).toString).flatMap(row=>{
      val uid = row.getInt(0).toString
      val items = row.getAs[Seq[Row]](1).map(item=>item.getInt(0).toString+":"+item.getDouble(1).formatted("%.4f")).mkString(",")
      val listBuffer = new ListBuffer[(ImmutableBytesWritable, KeyValue)]
      val kv1: KeyValue = new KeyValue(Bytes.toBytes(uid), Bytes.toBytes("f1"), Bytes.toBytes("als"), Bytes.toBytes(items))
      // 多个列按列名字典顺序append
      listBuffer.append((new ImmutableBytesWritable, kv1))
      listBuffer
    })
    hfileRdd
  }



}

object ALSModelData {
  def apply(spark: SparkSession, env: String): ALSModelData = new ALSModelData(spark, env)
}
```

#### ALSCF

```java
package com.qianfeng.recommend

import com.qianfeng.recommend.config.Config
import com.qianfeng.recommend.hbase.HBaseUtil
import com.qianfeng.recommend.transform.ALSModelData
import com.qianfeng.recommend.util.SparkHelper
import org.apache.log4j.{Level, Logger}
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.recommendation.{ALS, ALSModel}
import org.apache.spark.sql.{DataFrame, SaveMode}
import org.slf4j.LoggerFactory


/**
  * @Description: 基于模型的协同过滤 ALS 算法
  * @Author: QF    
  * @Date: 2020/7/20 5:54 PM   
  * @Version V1.0 
  */
object ALSCF {

  private val log = LoggerFactory.getLogger("als")

  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.WARN)
    System.setProperty("HADOOP_USER_NAME", "root")

    // 解析命令行参数
    val params = Config.parseConfig(ALSCF, args)
    log.warn("job running please wait ... ")
    // init spark session
    val ss = SparkHelper.getSparkSession(params.env, "als")
    // 防止ALS算法迭代次数过多，DAG过深，RDD的lineage过长，从而造成StackOverflowError异常
    ss.sparkContext.setCheckpointDir("/checkpoint/als")


    // 基础数据处理
    val modelData = ALSModelData(ss, params.env)

    // 将用户原始行为数据转换为评分数据
    val ratingDF = modelData.genUserRatingData()

    // 没有真实数据时，可以加载一些简单测试数据，运行算法流程
    //val ratingDF = modelData.someTestData()

    // 将训练数据分为训练集合测试集,4:1
    val Array(training, test) = ratingDF.randomSplit(Array(0.8, 0.2))

    //缓存训练数据集
    training.cache()

    // ALS 模型训练
    val als = new ALS()
      //交替最小二乘求解最大迭代次数
      .setMaxIter(6)
      // 正则化系数，避免过拟合
      .setRegParam(0.01)
      //用户列
      .setUserCol("uid")
      //物品列
      .setItemCol("aid")
      //评分列
      .setRatingCol("rating")
      // 当用训练出来的模型做预测时，如果测试集中包含了训练集中没有的用户或者物品(这在生成环境会很常见).对这些用户或物品做推荐时，如何取值
      // drop 策略表示这就删除掉对这些用户或物品的数据，Nan策略表示，用NaN表示这些值
      .setColdStartStrategy("drop")
      // 限制最小二乘解不出现负值
      .setNonnegative(true)
      //是否是隐式反馈数据集  -- 显示是显示隐式集
      .setImplicitPrefs(true)
      // 物品和用户特征的维度
      .setRank(16)

    // 训练模型
    val model:ALSModel = als.fit(training)

    // 在测试集上预测数据
    val predictions:DataFrame = model.transform(test)

    // 评估器，这里用来计算，预测出来的评分和原始测试集的评分的均方误差，来衡量训练出的模型优劣
    val evaluator = new RegressionEvaluator()
      .setMetricName("rmse")  //均方误差
      .setLabelCol("rating")  //真实的得分
      .setPredictionCol("prediction")  //预测得分
    // 计算均方误差
    val rmse = evaluator.evaluate(predictions)
    log.warn(s"[ALS算法] 均方根误差(Root-mean-square error) = $rmse")
    println(s"[ALS算法] 均方根误差(Root-mean-square error) = $rmse")

    // 为所有用户推荐topK结果，注意这里的结果包含用户已经有过行为的物品
    val userRecs: DataFrame = model.recommendForAllUsers(params.topK)

    // 从ALS结果中，过滤掉用户已经有过行为的物品
    val recoDF = modelData.filterALSRecommendForAllUser(ratingDF,userRecs)
    recoDF.show(false)

    // 推荐结果保存一份到到HDFS
    recoDF.write.mode(SaveMode.Overwrite).format("ORC").saveAsTable("dwb_news.als")
    //将已经过滤好的推荐列表转换数据格式
    val convertDF = modelData.recommendDataConvert(recoDF)

    // 结果存储到HBASE中
    val hBaseUtil =  HBaseUtil(ss,params.zkHost,params.zkPort)
    log.warn("start gen hfile, load als result to hbase ! ")
    // 将推荐结果的DataFrame转换为HFile RDD
    val hfileRDD = modelData.alsDF2HFile(convertDF)
    // HFile RDD 生成文件后直接加载到HBASE中
    hBaseUtil.loadHfileRDD2Hbase(hfileRDD,params.htableName,params.hfilePath)


    ss.stop()
    log.warn("job success! ")
  }
}

```



本地测试：

```properties
-e dev -h hadoop01 -p 2181 -f /recommend/als -k 100 -t recommend:news-cf
```



服务器提交：

```shell
# 编译打包
mvn clean package -DskipTests -Dscope.type=provided 
# 将编译好的JAR,recommend-1.0.0-jar-with-dependencies.jar,下载到你的服务器上
# 我放置到了 /opt/app/recommend/ 目录下
mkdir -p /opt/app/recommend/
# 同时也提供了一个编译好JAR，大家最好自己编译
wget -P  /opt/app/recommend/  http://doc.yihongyeyan.com/qf/project/soft/app/recommend-1.0.0-jar-with-dependencies.jar
# 之后执行如下命令，提交我们测程序到Yarn上，这次我们使用client方式提交作业，还记得之前数据落地程序吗，我们是采用cluster方式， 注意作业参数zk地址替换为你自己的地址
${SPARK_HOME}/bin/spark-submit \
--master yarn \
--deploy-mode client \
--conf spark.executor.heartbeatInterval=120s \
--conf spark.network.timeout=600s \
--conf spark.sql.catalogImplementation=hive \
--conf spark.sql.shuffle.partitions=20 \
--conf spark.yarn.submit.waitAppCompletion=true \
--name als \
--driver-memory 512M \
--num-executors 1 \
--executor-cores 1 \
--executor-memory 1G \
--class com.qf.bigdata.recommend.ALSCF \
/opt/app/recommend/recommend-1.0.0-jar-with-dependencies.jar \
-e dev -h hadoop01 -p 2181 -f /recommend/als -k 100 -t recommend:news-cf
```



# day02作业

1、总结itemCF和ALS召回模型

2、预习排序模型



## 2.5 物品基础向量

我们之前已经提到过，要对文章的基础信息(`也叫基础特征`)`文章字数`,`图片数量`,`文章的类型`，`发布时间距离计算的时间差`等转换为向量表示，为之后的排序阶段做特征输入。我们如何做这件事情呢，我们先来观察一下我们的特征，也就是这些字段，比`文章字数,图片数量，发布时间距离计算时间差`，这些特征都是`数值类型的`。`文章类型` 这个特征是`非数值的，我们也叫类别特征`。 如果我们把特征按照`数值特征`和`非数值特征[类别特征]`这两类来划分，现在我们问题就变成，如何将这两类特征进行向量话表示，幸运的是很多算法库`Spark mllib,python sklearn等`都提供了很多方法让我们来做`特征转换`, 术语叫`Feature Transformer`。那接下来我们使用这些方法对我们的文章信息进行向量化。



步骤：

对于文章基础信息我们从`ods_news.news_article`表中抽取，我们只抽取上述我们列举的四个特征作为例子讲解。我们依然通过SQL获取这四个特征`文章字数`,`图片数量`,`文章的类型`，`发布时间距离计算的时间差`， 创建表`dwb_news.article_base_info` 来存这些特征，我们选择全量的文章数据[`即今天之前的所天的文章数据`]。 大家可以动手写一写了。

```sql
-- 连接presto，执行如下SQL, 我们将此表建立到`dwb`层,命名为`dwb_news.article_base_info`
-- 此SQL提取文章的基础信息数据  `文章字数`,`图片数量`,`文章的类型`，`发布时间距离计算的时间差`, 如果有相同的article_id, 只保留一个，其他字段都统一取最大值
create table dwb_news.article_base_info
comment 'article base info '
with (format = 'ORC')
as 
with t1 as (
select 
  article_id,
  length(array_join(regexp_extract_all(content,'[\u4e00-\u9fa5]+'),';')) as article_num,
  (length(content) - length(replace(content,'<img src=','')))/length('<img src=') as img_num,
  type_name,
  date_diff('day',cast(split(pub_time,'T')[1] as date),cast(now() as date)) as pub_gap
from ods_news.news_article
where logday< format_datetime(now(),'yyyyMMdd')
and article_id <>'' and content <> ''
)
select article_id,max(article_num) as article_num ,
max(img_num) as img_num,
max(type_name) as type_name,
min(pub_gap) as pub_gap
from t1 
group by article_id;

-- 执行查询验证数据是否生成
select * from dwb_news.article_base_info limit 5;
```



### 特征转换

有了基础数据，我们接下来就对如上数据做特征的转换，首先对于数值型的特征，我们可以看到`article_num` ，`img_num`,`pub_gap` 这三列数据，量纲都不一致，比如``article_num``这个特征，数值相比其他两个都比较大，如果我们直接使用而不做变换，会影响我们后边排序层算法的训练，这样说大家可能不太理解，这里面蕴含的数学和算法知识比较多。我们先简单的从另一个角度来思考，比如我们现在一篇文章有 `article_num` 和`img_num` 这两个特征，比如A文章`(2178,14)`, B文章`(348，4)` ， 当我们计算两篇的文章的距离时，比如欧式距离，`距离=`$\sqrt{(2178-348)^2+(14-4)^2}$,你会发现这个距离的值基本靠第一个特征也就是`article_num` 这个特征决定，第二个特征`img_num` 基本不起作用，这肯定不是我们想要的结果。因此我们应该把他们做一个数学变换，缩放到一个合适的空间，来消除由于不同特征度量不同而产生的问题。 我们可以选择`MinMaxScaler` ，我们叫它最大最小归一化，它的数学公式也很简单

$x_{i}^{'} = \dfrac{x_{i}-min(x)}{max(x)-min(x)}$

我们来解释一下，其中`x` 表示我们的特征，$x_{i}$ 表示一个特征的一个值， $min(x)$ 表示这个`x`特征的最小值, $max(x)$ 表示`x` 特征的最大值。 我们可以发现数据经过这样的处理收，都会在[0-1] 区间, 对于特殊情况`最大值=最小值` 时，$x_{i}^{'}$ 为0.5。 我们就以上一小节查出来的`article_num` 这列数据的5条数据做个例子

```markdown
# 数据
 article_id | article_num |
------------+-------------+
 73789      |        2178 |
 73809      |         348 |
 73355      |        1297 |
 73384      |         356 |
 73389      |        1192 |
 
* 最小值 = 348
* 最大值 = 2178
* 根据公式
1. 对于article_id=73789 的article_num值其归一化后 =  （2178 - 348)/2178-348 = 1
2. 对于article_id=73809 的article_num值其归一化后 =  （348 - 348)/2178-348 = 0
3. 对于article_id=73355 的article_num值其归一化后 =  （1297 - 348)/2178-348 = 0.51
4. 对于article_id=73384 的article_num值其归一化后 =  （356 - 348)/2178-348 = 0.004
5. 对于article_id=73389 的article_num值其归一化后 =  （1192 - 348)/2178-348 = 0.46

# 转换后数据
 article_id | article_num |
------------+-------------+
 73789      |        1    |
 73809      |        0    |
 73355      |        0.51 |
 73384      |        0.004|
 73389      |        0.46 |
```

经过以上例子，大家应该明白了最大最小归一化如何计算，对于其他两列数值型的特征我们也使用同样的方式做数据变换就可以了。

> 对于`type_name` 这个非数值类型特征，也就是类别特征我们使用`OneHotEncoder` 独热编码来将其进行转换

对于``OneHotEncoder` 其实是一个非常简单的事情，我举个例子大家就明白了，对于`type_name` 这个特征，我们先统计这个特征下有多少不同的值，比如有`5`个不同的值，那这个特征将会被表示为`5`维的一个向量，`(0,0,0,0,0)`, 然后对于其中的一个值，如果在向量对应的位置上有值，我们就设置为1，其他都为0,看下面的例子

```markdown
# 数据
 article_id | type_name |
------------+-----------+
 73789      | 娱乐      |  
 73809      | 娱乐      |  
 73355      | 情感      |  
 73384      | 情感      |  
 73389      | 情感      |  
 
* 对于`type_name` 这列数据一共有，两个不同的值， [娱乐, 情感]，type_name 这个特征就可以用 两维向量表示
* 对于article_id=73789 ，表示为 [1,0]
* 对于article_id=73789,  表示为 [1,0]
* 对于article_id=73355 , 表示为 [0,1]
* 对于article_id=73384, 表示为  [0,1]
* 对于article_id=73389， 表示为  [0,1]

# 转换后的数据
 article_id | type_name |
------------+-----------+
 73789      | [1,0]    |  
 73809      | [1,0]    |  
 73355      | [0,1]    |  
 73384      | [0,1]    |  
 73389      | [0,1]    |  
```

通过例子大家可以看到，这是非常简单的一件事，但是这里要说一下如果一个类别特征的`基数[也就是不同的值]`特别多，你会发现这个特征用我们上述``OneHotEncoder`的方式表示就会出现`维度爆炸的问题[也就是维度特别多，并且只有一个维度有值为1，其它都是0`]。对于这个问题我们在机器学习的过程中是要避免的，但这不是我们本项目中要讨论的问题。



### Item Base Feature To Hbase

接下来我们要做的就是对上述我们的讲解用`Spark MLlib` 实现，我们先列一下我们要做的事项

1. 从``dwb_news.article_base_info`` 读取我们之前抽取的文章信息数据
2. 对其中`article_num` ，`img_num`,`pub_gap` 列使用`MinMaxScaler`做归一化处理
3. 对其中`type_name`列使用`OneHotEncoder` 进行独热编码
4. 将经过处理后的文章各列的特征拼接，以文章ID为`RowKey` 存入到`HBASE`中

对于第四点说明如下

```shell
# 因为我们会将最终的结果存入到HBASE中，我们先来创建一个HBASE表
# 进入到HBASE SHELL 执行如下命令
# 创建 news-feature 表，存放物品基本特征向量
create 'recommend:news-feature','f1'

# 我们代码中会将文章特征向量写入到该表中，列族 f1， 列名为 base
```

#### ItemBaseFeatureModelData

```java
package com.qianfeng.recommend.transform

import org.apache.hadoop.hbase.KeyValue
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.rdd.RDD
import org.apache.spark.sql
import org.apache.spark.sql.{Row, SparkSession}

import scala.collection.mutable.ListBuffer

/**
  * @Description: 为物品基础特征转换提供数据处理方法
  * @Author: QF    
  * @Date: 2020/7/24 5:27 PM   
  * @Version V1.0 
  */
class ItemBaseFeatureModelData(spark: SparkSession, env:String) extends ModelData(spark: SparkSession, env:String) {


  /**
    * ItemBaseFeature 产生的结果生成HFile RDD
    * 需要添加建立好hbase表，行键将会存入uid, 列族为 f1 , 列为 base ,值为物品特征的向量
    * 创建hbase表样例如：
    *   create_namespace 'recommend'
    *   create 'recommend:news-feature','f1'
    * @param itemBaseFeatureDF  生成物品特征ItemBaseFeature向量，格式[article_id,SparseVector.toString]
    * @return
    */
  def itemBaseFeatureDF2HFile(itemBaseFeatureDF:sql.DataFrame):RDD[(ImmutableBytesWritable,KeyValue)]={

    val hfileRdd = itemBaseFeatureDF.rdd.sortBy(x =>x.get(0).toString).flatMap(row=>{
      val itemId = row.getString(0)
      val features = row.getString(1)
      val listBuffer = new ListBuffer[(ImmutableBytesWritable, KeyValue)]
      val kv1: KeyValue = new KeyValue(Bytes.toBytes(itemId), Bytes.toBytes("f1"), Bytes.toBytes("base"), Bytes.toBytes(features))
      // 多个列按列名字典顺序append
      listBuffer.append((new ImmutableBytesWritable, kv1))
      listBuffer
    })
    hfileRdd
  }



}


object ItemBaseFeatureModelData{
  def apply(spark: SparkSession, env: String): ItemBaseFeatureModelData = new ItemBaseFeatureModelData(spark, env)
}
```

#### FeatureUDF

```java
package com.qianfeng.recommend.udfs

import org.apache.commons.lang3.StringUtils
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions.udf

/**
  * @Description: 为数据特征转换提供函数
  * @Author: QF    
  * @Date: 2020/8/3 6:10 PM   
  * @Version V1.0 
  */
object FeatureUDF {


  /**
    * 多个Vector String列合并为一列，例如
    * "[1,2]" "[4,5]"
    * 合并后 [1,2,4,5]
    * @param row ROW eg: struct($"col1",$"col2")
    * @return string
    */
  def mergeRow(row: Row): String = {

    row.toSeq.foreach(line=>StringUtils.strip(line.toString,"[]"))
    val res = row.toSeq.foldLeft("")((x,y) => StringUtils.strip(x,"[]")+ ","+StringUtils.strip(y.toString,"[]")).substring(1)
    "["+res+"]"
  }
  // 多个Vector String列合并为一列
  val mergeCols = udf(mergeRow _)

  // 向量转array字符串
  val vector2Str = udf((vecStr: org.apache.spark.ml.linalg.Vector) => {
    //向量转换为稠密向量，稠密向量转字符串
    vecStr.toDense.toString()
  })

  // 向量字符串转向量求向量维度
   val  vecStr2Size = udf((vecStr: String) => {
    Vectors.parse(vecStr).asML.size
  })

  // 向量字符串转向量
  val vecStr2Vec = udf((vecStr: String) => {
    Vectors.parse(vecStr).asML
  })

}

```

#### ItemBaseFeature

```java
package com.qianfeng.recommend

import breeze.linalg.DenseVector
import com.qianfeng.recommend.config.Config
import com.qianfeng.recommend.hbase.HBaseUtil
import com.qianfeng.recommend.transform.ItemBaseFeatureModelData
import com.qianfeng.recommend.udfs.FeatureUDF
import com.qianfeng.recommend.util.SparkHelper
import org.apache.log4j.{Level, Logger}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.{MinMaxScaler, OneHotEncoderEstimator, StringIndexer, VectorAssembler}
import org.apache.spark.ml.linalg.{SparseVector, Vectors}
import org.apache.spark.sql.{DataFrame, SaveMode}
import org.apache.spark.sql.functions.udf
import org.slf4j.LoggerFactory

/**
  * @Description:  生成文章基本特征向量，并存储到HBASE中
  * @Author: QF    
  * @Date: 2020/7/24 5:20 PM   
  * @Version V1.0 
  */
object ItemBaseFeature {
  private val log = LoggerFactory.getLogger("item-base-feature")

  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.WARN)
    System.setProperty("HADOOP_USER_NAME", "root")

    // 解析命令行参数
    val params = Config.parseConfig(ItemBaseFeature, args)
    log.warn("job running please wait ... ")
    // init spark session
    val ss = SparkHelper.getSparkSession(params.env, "item-base-feature")
    // 基础数据处理
    val modelData = ItemBaseFeatureModelData(ss, params.env)

    // 从dwb_news.article_base_info数据表中读取文章特征数据
    val itemFeatureDF:DataFrame = modelData.loadSourceArticleBaseInfoData()
    /**
      * 对对itemFeatureDF中的`type_name`列，做one-hot编码
      * 因为 spark OneHotEncoderEstimator 只接受数值类型列作为one-hot编码的输入，因此我们先使用 StringIndexer将type_name的中文类型类
      * 转换为对应的索引，StringIndexer 的作用举例如下
      * type_name
      * ------------
      * 娱乐
      * 情感
      * 汽车
      * 娱乐
      *
      * 执行StringIndexer后
      * type_name
      * ------------
      * 0
      * 1
      * 2
      * 0
      * 相当于每一个值有一个索引，以索引号代替数值
      *
      */
    val indexer = new StringIndexer()
      .setInputCol("type_name")  //输入列
      .setOutputCol("type_name_index")  //输出列
    // 对转换为索引表示的type_name的使用one-hot编码，这里需要注意的是，转换索引操作并不影响我们one-hot编码的结果
    // 之所以做转换，是因为spark的 OneHotEncoderEstimator 只接受数值类型作为输入
    val encoder = new OneHotEncoderEstimator()
       .setInputCols(Array(indexer.getOutputCol)) //设置输入列
       .setOutputCols(Array("type_name_vec"))  //输出列

    /** 三个数值类型特征合并为一个向量，举例如下
      *
      * article_num | img_num| pub_gap
      * -------------|--------|------------
      * 356         | 18     |4
      *
      * 对上述三列执行 VectorAssembler 后会得到如下结果
      *
      * features
      * ---------
      * [356,18,4]
      */
    val numAssembler = new VectorAssembler()
      .setInputCols(Array("article_num", "img_num", "pub_gap"))  //设置合并的输入的列
      .setOutputCol("numFeatures")  //输出列

    // 对各个列进行最小最大归一化
    val numFeaturesScaler = new MinMaxScaler()
      .setInputCol("numFeatures")
      .setOutputCol("numFeaturesScaler")

    // 合并归一化后的数值特征列[article_num,img_num,pub_gap]和one-hot后的type_name列合并为一个向量
    val assembler = new VectorAssembler()
      .setInputCols(Array("numFeaturesScaler", "type_name_vec"))  //合并的输入列
      .setOutputCol("features")  //输出列

    //定义一个Pipeline,将各个特征转换操作放入到其中处理
    val pipeline = new Pipeline()
      .setStages(Array(indexer, encoder, numAssembler, numFeaturesScaler, assembler))

    // 将数据集itemFeatureDF，就是我们的文章内容信息数据作用到我们定义的pipeline上
    val pipelineModel = pipeline.fit(itemFeatureDF)  //训练
    // 对我们的数据集执行定义的转换操作
    val featuresDF = pipelineModel.transform(itemFeatureDF)  //测试

    featuresDF.show(false)
    import ss.implicits._

    //  将features列SparseVector转换为string,保留将文章ID和其对应的特征向量两列
    val baseFeatureDF = featuresDF
      .withColumn("features", FeatureUDF.vector2Str($"features"))
      .select("article_id", "features")

    // 将特征向量保存一份到到HDFS
    baseFeatureDF.write.mode(SaveMode.Overwrite).format("ORC").saveAsTable("dwb_news.article_base_vector")

    // 将特征向量存入到HBASE,先将其转换为HFile
    val hBaseUtil = HBaseUtil(ss, params.zkHost, params.zkPort)
    log.warn("start gen hfile, load item base feature result to hbase ! ")
    // 物品特征的DataFrame转换为HFile RDD
    val featureHFileRDD = modelData.itemBaseFeatureDF2HFile(baseFeatureDF)
    // HFile RDD 生成文件后直接加载到HBASE中
    hBaseUtil.loadHfileRDD2Hbase(featureHFileRDD, params.htableName, params.hfilePath)
    ss.stop()
    log.warn("job success! ")
  }
}

```



本地测试：

```properties
-e dev -h hadoop01 -p 2181 -t recommend:news-feature -f /recommend/itembase
```



## 2.6 用户基础标签

用户基础标签获取：(可以直接从画像基础标签表中获取)

```sql
-- 执行如下sql
create table dwb_news.user_base_feature
comment 'user base feature from ods_news.user '
with (format = 'ORC')
as 
select 
distinct_id as uid,
gender,
age, 
case when cardinality(split(email,'@'))=2 then split(email,'@')[2]
else '' end as email_suffix
from ods_news.user 

-- 查询该表数据，结果类似如下
select * from dwb_news.user_base_feature limit 5;
```



### user Base Feature To Hbase

我们列一下我们要做的工作事项

1. 从上一小节生成的`dwb_news.user_base_feature` 表中加载用户数据
2. 对`gender,email_suffix`做`OneHotEncoder`
3. 对`age` 特征离散化表示
4. 合并三个特征的向量，将用户向量存储到`HBASE` 中

```properties
# 对于存储到HBASE中用户向量，我们先创建一个HBASE表
create 'recommend:user-feature','f1'

# 会在代码中将向量存到该表中，行键为用户ID， 列族 f1 ,列名 base， 值为用户向量的稀疏表示形式
```

#### UserBaseFeatureModelData

```java
package com.qianfeng.recommend.transform

import org.apache.hadoop.hbase.KeyValue
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.rdd.RDD
import org.apache.spark.sql
import org.apache.spark.sql.SparkSession

import scala.collection.mutable.ListBuffer

/**
  * @Description: 为用户基础特征转换提供数据处理方法
  * @Author: QF    
  * @Date: 2020/7/28 12:40 PM   
  * @Version V1.0 
  */
class UserBaseFeatureModelData(spark: SparkSession, env:String) extends ModelData(spark: SparkSession, env:String) {

  /**
    * UserBaseFeature 产生的结果生成HFile RDD
    * 需要添加建立好hbase表，行键将会存入uid, 列族为 f1 , 列为 base ,值为物品特征的向量
    * 创建hbase表样例如：
    *   create_namespace 'recommend'
    *   create 'recommend:user-feature','f1'
    * @param userBaseFeatureDF  生成的用户特征UserBaseFeature向量，格式[uid,SparseVector.toString]
    * @return
    */
  def userBaseFeatureDF2HFile(userBaseFeatureDF:sql.DataFrame):RDD[(ImmutableBytesWritable,KeyValue)]={

    val hfileRdd = userBaseFeatureDF.rdd.sortBy(x =>x.get(0).toString).flatMap(row=>{
      val itemId = row.getString(0)
      val features = row.getString(1)
      val listBuffer = new ListBuffer[(ImmutableBytesWritable, KeyValue)]
      val kv1: KeyValue = new KeyValue(Bytes.toBytes(itemId), Bytes.toBytes("f1"), Bytes.toBytes("base"), Bytes.toBytes(features))
      // 多个列按列名字典顺序append
      listBuffer.append((new ImmutableBytesWritable, kv1))
      listBuffer
    })
    hfileRdd
  }

}


object UserBaseFeatureModelData{
  def apply(spark: SparkSession, env: String): UserBaseFeatureModelData = new UserBaseFeatureModelData(spark, env)
}
```



#### UserBaseFeature

```java
package com.qianfeng.recommend

import com.qianfeng.recommend.config.Config
import com.qianfeng.recommend.hbase.HBaseUtil
import com.qianfeng.recommend.transform.UserBaseFeatureModelData
import com.qianfeng.recommend.udfs.FeatureUDF
import com.qianfeng.recommend.util.SparkHelper
import org.apache.log4j.{Level, Logger}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature._
import org.apache.spark.ml.linalg.SparseVector
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions.udf
import org.slf4j.LoggerFactory

/**
  * @Description: 生成用户基本特征向量，并存储到HBASE中
  * @Author: QF    
  * @Date: 2020/7/28 12:37 PM   
  * @Version V1.0 
  */
object UserBaseFeature {

  private val log = LoggerFactory.getLogger("user-base-feature")

  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.WARN)
    System.setProperty("HADOOP_USER_NAME", "root")

    // 解析命令行参数
    val params = Config.parseConfig(UserBaseFeature, args)
    log.warn("job running please wait ... ")
    // init spark session
    val ss = SparkHelper.getSparkSession(params.env, "user-base-feature")
    // 基础数据处理
    val modelData = UserBaseFeatureModelData(ss, params.env)

    // 读取用户特征数据
    val userFeatureDF = modelData.loadSourceUserBaseFeature()

    // 为gender列值创建索引表示
    val genderIndexer = new StringIndexer()
      .setInputCol("gender").setOutputCol("gender_index")
    // 为email列创建索引表示
    val emailIndexer = new StringIndexer()
      .setInputCol("email_suffix").setOutputCol("email_index")

    // 对gender email 特征进行one-hot 编码
    val encoder = new OneHotEncoderEstimator()
      .setInputCols(Array(genderIndexer.getOutputCol, emailIndexer.getOutputCol))
      .setOutputCols(Array("gender_vec", "email_vec"))

    // 对于age特征我们划分年龄段为 [0,15,,25,35,50,60] 表示 0-15, 15-25 依次类推
    // Bucketizer 就可以帮我们做这件事
    val splits = Array(0, 15, 25, 35, 50, 60, Double.PositiveInfinity)
    val bucketizer = new Bucketizer()
      .setInputCol("age")
      .setOutputCol("age_buc")
      .setSplits(splits)

    // 对bucket 后的 age 列进行最小最大归一化
    val ageAssembler = new VectorAssembler()
      .setInputCols(Array("age_buc"))
      .setOutputCol("age_vec")

    val featuresScaler = new MinMaxScaler()
      .setInputCol("age_vec")
      .setOutputCol("age_scaler")

    // 合并归一化后的数值特征列[article_num,img_num,pub_gap]和one-hot后的type_name列合并为一个向量
    val assembler = new VectorAssembler()
      .setInputCols(Array("gender_vec", "email_vec", "age_scaler"))
      .setOutputCol("features")

    // 定义一个Pipeline,将各个特征转换操作放入到其中处理
    val pipeline = new Pipeline()
      .setStages(Array(genderIndexer, emailIndexer, encoder, bucketizer, ageAssembler, featuresScaler, assembler))

    // 将数据集itemFeatureDF，就是我们的文章内容信息数据作用到我们定义的pipeline上
    val pipelineModel = pipeline.fit(userFeatureDF)  //训练
    // 对我们的数据集执行定义的转换操作
    val featuresDF = pipelineModel.transform(userFeatureDF)  //测试
    featuresDF.show(false)
    import ss.implicits._


    //  将features列SparseVector转换为string,保留将文章ID和其对应的特征向量两列
    val baseFeatureDF = featuresDF
      .withColumn("features",  FeatureUDF.vector2Str($"features"))
      .select("uid", "features")

    // 将特征向量保存一份到到HDFS
    baseFeatureDF.write.mode(SaveMode.Overwrite).format("ORC").saveAsTable("dwb_news.user_base_vector")

    // 将特征向量存入到HBASE,先将其换换为HFile
    val hBaseUtil = HBaseUtil(ss, params.zkHost, params.zkPort)
    log.warn("start gen hfile, load user base feature result to hbase ! ")
    // 将用户特征的DataFrame转换为HFile RDD
    val featureHFileRDD = modelData.userBaseFeatureDF2HFile(baseFeatureDF)
    // HFile RDD 生成文件后直接加载到HBASE中
    hBaseUtil.loadHfileRDD2Hbase(featureHFileRDD, params.htableName, params.hfilePath)
    ss.stop()
    log.warn("job success! ")

  }
}

```



本地测试：

```properties
-e dev -h hadoop01 -p 2181 -t recommend:user-feature -f /recommend/userbase
```





## 2.7 WordEmbedding 



1. 加载`dwb_news.article_top_terms_w2v` 此表数据(如果画像项目后，你未更新该表数据，请重新生产该表数据)，按文章ID分组，词向量相加求平均

2. 将文章向量[`Embedding`]写入到HBASE中

   ```markdown
   # 我们已经有了文章的基础属性特征向量在HBASE的 `recommend:news-feature` 表的，f1 列族的 base列下
   # 我们依然将生产的文章Embedding向量也写入到此表中，列族依然是 f1, 列名为 embedding
   ```

#### ArticleEmbeddingModelData

```java
package com.qianfeng.recommend.transform

import org.apache.hadoop.hbase.KeyValue
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.rdd.RDD
import org.apache.spark.sql
import org.apache.spark.sql.SparkSession

import scala.collection.mutable.ListBuffer

/**
  * @Description: 为文章向量生产提供数据转换方法
  * @Author: QF    
  * @Date: 2020/7/28 4:41 PM   
  * @Version V1.0 
  */
class ArticleEmbeddingModelData(spark: SparkSession, env:String) extends ModelData(spark: SparkSession, env:String) {


  /**
    * 从数仓dwb_news.article_top_terms_w2v 表中读取文章关键词向量数据
    *
    * article_id | top_term | vector
    * ------------+----------+--------------------------------------------------------------------------------------------------------------------------------------------
    * 8862607    | 减速慢行 | [0.03496983274817467, 0.17339648306369781, -0.11213770508766174, -0.02299109846353531, 0.09417295455932617, -0.013225400820374489, 0.200927
    * 8862607    | 弱者     | [-0.08087116479873657, -0.047462671995162964, 0.026697132736444473, -0.030793823301792145, 0.06060231477022171, 0.056847646832466125, -0.07
    * 8862606    | 设计师   | [-0.22160547971725464, -0.02786044403910637, -0.14247123897075653, -0.15862725675106049, -0.27565187215805054, -0.02081381343305111, 0.1681
    * 8862606    | 美感     | [-0.1846056580543518, -0.2657729983329773, -0.09711218625307083, -0.27734848856925964, -0.5568212270736694, 0.06630297005176544, 0.01408153
    *
    * @return
    */
  def loadArticleTermsVector(): sql.DataFrame = {

    val loadSourceSql =
      s"""
         |select
         |article_id,
         |top_term,
         |vector
         |from dwb_news.article_top_terms_w2v
         |where vector is not null
      """.stripMargin

    spark.sql(loadSourceSql)
  }


  /**
    * ArticleEmbedding 产生的结果生成HFile RDD
    * 需要添加建立好hbase表，行键将会存入uid, 列族为 f1 , 列为 embedding ,值为物品特征的向量
    * 创建hbase表样例如：
    *   create_namespace 'recommend'
    *   create 'recommend:news-feature','f1'
    * @param articleEmbeddingDF  生成物品特征ItemBaseFeature向量，格式[article_id,SparseVector.toString]
    * @return
    */
  def articleEmbeddingDF2HFile(articleEmbeddingDF:sql.DataFrame):RDD[(ImmutableBytesWritable,KeyValue)]={

    val hfileRdd = articleEmbeddingDF.rdd.sortBy(x =>x.get(0).toString).flatMap(row=>{
      val itemId = row.getString(0)
      val features = row.getString(1)
      val listBuffer = new ListBuffer[(ImmutableBytesWritable, KeyValue)]
      val kv1: KeyValue = new KeyValue(Bytes.toBytes(itemId), Bytes.toBytes("f1"), Bytes.toBytes("embedding"), Bytes.toBytes(features))
      // 多个列按列名字典顺序append
      listBuffer.append((new ImmutableBytesWritable, kv1))
      listBuffer
    })
    hfileRdd
  }
}

object ArticleEmbeddingModelData {
  def apply(spark: SparkSession, env: String): ArticleEmbeddingModelData = new ArticleEmbeddingModelData(spark, env)
}
```



#### ArticleEmbedding

```java
package com.qianfeng.recommend

import com.qianfeng.recommend.config.Config
import com.qianfeng.recommend.hbase.HBaseUtil
import com.qianfeng.recommend.transform.ArticleEmbeddingModelData
import com.qianfeng.recommend.udfs.FeatureUDF
import com.qianfeng.recommend.util.SparkHelper
import org.apache.log4j.{Level, Logger}
import org.apache.spark.ml.linalg.{DenseVector, SparseVector, Vectors}
import org.apache.spark.ml.stat.Summarizer
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions.udf
import org.slf4j.LoggerFactory

/**
  * @Description: 生产文章Embedding向量,写入到HBASE
  * @Author: QF    
  * @Date: 2020/7/28 4:39 PM   
  * @Version V1.0 
  */
object ArticleEmbedding {


  private val log = LoggerFactory.getLogger("article-embedding")

  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.WARN)
    System.setProperty("HADOOP_USER_NAME", "root")

    // 解析命令行参数
    val params = Config.parseConfig(ArticleEmbedding, args)
    log.warn("job running please wait ... ")
    // init spark session
    val ss = SparkHelper.getSparkSession(params.env, "article-embedding")
    // 基础数据处理
    val modelData = ArticleEmbeddingModelData(ss, params.env)
    // 读取用户特征数据
    val articleTermsVectorDF = modelData.loadArticleTermsVector()

    // 定义函数，将array[double] 转换为稠密向量, 因为我们之前存到表里的词向量是array[double]类型的
    // 这里将字符串数组转为稠密向量DenseVector，方便计算
    val array2vec = udf((array: Seq[Double]) => {
      Vectors.dense(array.toArray)
    })

    import ss.implicits._
    // 以文章ID为维度，将所有该文章下的所有词向量加和求平均得到文章向量
    val articleEmbeddingDF = articleTermsVectorDF
      .withColumn("vector", array2vec($"vector"))
      .groupBy("article_id")  //article_id分组
      .agg(Summarizer.mean($"vector").alias("article_vector"))


    //  将features列SparseVector转换为string,保留将文章ID和其对应的特征向量两列
    val embeddingDF = articleEmbeddingDF
      .withColumn("article_vector", FeatureUDF.vector2Str($"article_vector"))
      .select("article_id", "article_vector")

    // 将特征向量保存一份到到HDFS
    embeddingDF.write.mode(SaveMode.Overwrite).format("ORC").saveAsTable("dwb_news.article_embedding")

    // 将特征向量存入到HBASE,先将其换换为HFile
    val hBaseUtil = HBaseUtil(ss, params.zkHost, params.zkPort)
    log.warn("start gen hfile, load article embedding  result to hbase ! ")
    // EmbeddingDataFrame 转换为HFile RDD
    val embeddingHFileRDD = modelData.articleEmbeddingDF2HFile(embeddingDF)
    // HFile RDD 生成文件后直接加载到HBASE中
    hBaseUtil.loadHfileRDD2Hbase(embeddingHFileRDD, params.htableName, params.hfilePath)
    ss.stop()
    log.warn("job success! ")
  }
}
```

本地测试：

```properties
-e dev -h hadoop01 -p 2181 -t recommend:news-feature -f /recommend/articleembedding
```



## 2.8 UserEmbedding

对于`UserEmbedding` 我们在`用户画像`项目中已经通过用户行为和文章向量生成了一种用户`Embedding`的表示形式，我们这里不再介绍其它的表示形式了，同时对于`UserEmbedding`的向量我们也不在排序阶段使用，只使用我们生成的用户基本特征向量。 我们在排序阶段不使用用户的Embedding向量，并不是说不能用，只是这里我们`画像项目中`生成的用户`Embedding`，本质上还是有文章的`Embedding`而来。我们只使用文章的`Embedding`向量作为排序阶段的特征是可以的。这里要表达的是，到底用不用`UserEmbedding`向量，其实是要看我们排序模型算法的测试效果而定的。如果使用了效果没有明显提升，就不使用了，反之就可以加入`UserEmbedding`向量。



## 2.9 Ranking





```properties
# 我们想将第一个步骤中的样本数据准备好，这个很简单，用SQL查询实现即可。 为了加快计算我们选择最近三天的数据进行训练，你可以选择更长的时间， 公司中一般根据其自身产品特点选择，可以是`一个月`，`三个月`，`半年`，`一年等更长时间`。更多样本数据意味着更长的训练时间，消耗更多的资源，模型可能会有更好的表现。
```

```sql
--- 执行如下SQL生成训练数据
create table dwb_news.user_item_training
comment 'article base info '
with (format = 'ORC')
as 
with t1 as (
  select distinct_id as uid , 
  article_id as aid, 
  '浏览' as action,
  logday as action_date
  from ods_news.event
  where event in ('AppPageView') 
  and element_page in ('新闻列表页','内容详情页')
  and logday>=format_datetime(now()- interval '60' day,'yyyyMMdd') 
  and  logday< format_datetime(now(),'yyyyMMdd')
  and article_id <>''
),t2 as (
  select distinct_id as uid , 
  article_id as aid, 
  '点击' as action,
  logday as action_date
  from ods_news.event
  where event in ('AppPageView') 
  and element_page in ('内容详情页')
  and logday>=format_datetime(now()- interval '60' day,'yyyyMMdd') 
  and logday< format_datetime(now(),'yyyyMMdd')
  and article_id <>''
),t3 as (
  select t1.uid,t1.aid,t1.action as view,t2.action as click,
  case when t2.aid is null then  0
  else 1 end as label
  from t1 
  left join t2 on 
  t1.aid = t2.aid and t1.uid=t2.uid
)select uid,aid,view,click,label from t3 order by uid, view,aid;


-- 查询生成的数据会看到如下类似结果
select * from dwb_news.user_item_training limit 100;
```



### LR的模型

#### LRModelData

```java
package com.qianfeng.recommend.transform

import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegressionModel}
import org.apache.spark.sql
import org.apache.spark.sql.SparkSession

/**
  * @Description: 逻辑回归模型数据处理
  * @Author: QF    
  * @Date: 2020/7/29 5:06 PM   
  * @Version V1.0 
  */
class LRModelData(spark: SparkSession, env: String) extends ModelData(spark: SparkSession, env: String) {

  /**
   * 查询用户、文章、lable的训练数据
   * @return
   */
  def getVectorTrainingData(): sql.DataFrame = {
    val vectorSql =
      s"""
          with t1 as (
          select uid,aid,label from dwb_news.user_item_training
          ) ,
          t2 as (
          select
          t1.*,
          ubv.features as user_features
          from t1
          left join dwb_news.user_base_vector as ubv
          on t1.uid = ubv.uid
          where ubv.uid is not null and ubv.features <> ''
          ) ,
          t3 as (
          select
          t2.*,
          abv.features as article_features
          from t2
          left join dwb_news.article_base_vector as abv
          on t2.aid = abv.article_id
          where abv.article_id is not null and abv.features <> ''
          ),
          t4 as (
          select
          t3.*,
          ae.article_vector as article_embedding
          from t3
          left join dwb_news.article_embedding as ae
          on t3.aid = ae.article_id
          where ae.article_id is not null and ae.article_vector <> ''
          )
          select
          uid as uid,
          aid as aid,
          user_features as user_features,
          article_features as article_features,
          article_embedding as article_embedding,
          cast(label as int) as label
          from t4
      """.stripMargin

    spark.sql(vectorSql)
  }

  /**
   * 打印模型信息
   * @param lrModel
   */
  def printSummary(lrModel: LogisticRegressionModel): Unit = {
    val trainingSummary = lrModel.summary
    // 获取每个迭代目标函数的值
    val objectiveHistory = trainingSummary.objectiveHistory
    println("objectiveHistory:")
    objectiveHistory.zipWithIndex.foreach{case (loss,iter) =>{
      println(s"iterator: ${iter}, loss: ${loss}")
//      println(loss)
    }}

    println(s"accuracy:" +trainingSummary.accuracy)
    // 获取模型评估指标来衡量模型的表现
    val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]
    // 获取roc DataFrame
//     val roc = binarySummary.roc
//     roc.show()
    // 打印模型的AUC
    println(s"auc: ${binarySummary.areaUnderROC}")
  }


}


object LRModelData {
  def apply(spark: SparkSession, env: String): LRModelData = new LRModelData(spark, env)
}
```

#### VectorSchema

```java
package com.qianfeng.recommend.transform

import com.qianfeng.recommend.udfs.FeatureUDF
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions.{col, udf, _}
import org.apache.spark.sql.types.{DoubleType, StructField, StructType}

import scala.collection.mutable.ArrayBuffer

/**
  * @Description: DataFrame 的向量字符串列schema转换
  * @Author: QF    
  * @Date: 2020/8/3 2:28 PM   
  * @Version V1.0 
  */
class VectorSchema {

  /**
    * 根据指定DataFrame 的向量字符串列，将其向量的每一个值设定一个字段，即获取指定向量字符串列的Schema
    * 例如：col1 =  "[0,1,2,3]"
    * 或得到 f1,f2,f3,f4 四个列
    * @param df
    * @param arrayCols
    * @return
    */
  def getVectorSchemaByStrColumns(df:DataFrame,arrayCols: Array[String]): org.apache.spark.sql.types.StructType ={

    val featureCol = ArrayBuffer[String]()
    var a = 0
    val featureSchema = ArrayBuffer[StructField]()
    for ((colName,index) <- arrayCols.zipWithIndex){

      val outColSizeName = colName+index
      val arrayROW = df.withColumn(outColSizeName,FeatureUDF.vecStr2Size(col(colName)))
        .select(outColSizeName).head(1)
      val size = arrayROW(0).getAs[Int](outColSizeName)

      for( a <- 1 to size){
        val feature = "f"+a
        featureCol.append(feature)
        featureSchema.append(StructField(feature,DoubleType,true))
      }
    }

    StructType(featureSchema.toList)

  }
}

object VectorSchema{
  def apply: VectorSchema = new VectorSchema()
}
```

#### StringVector

```java
package org.apache.spark.ml.feature

import org.apache.commons.lang3.StringUtils
import org.apache.spark.annotation.Since
import org.apache.spark.ml.Transformer
import org.apache.spark.ml.linalg.VectorUDT
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.ml.param.shared._
import org.apache.spark.ml.util._
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.{DataFrame, Dataset}

import scala.language.existentials

/**
  * @Description:  自定义向量字符串转向量的Transformer, 只对输入的vector string列做转换，且输入列必须有列名`features`
  * @Author: QF    
  * @Date: 2020/7/31 5:52 PM   
  * @Version V1.0 
  */

class StringVector(override val uid: String) extends Transformer with HasInputCols with HasOutputCol{

  def setInputCol(value: Array[String]): this.type = set(inputCols, value)

//  def setInputCol(value: String): this.type = set(inputCol, value)

  def setOutputCol(value: String): this.type = set(outputCol, value)

  def this() = this(Identifiable.randomUID("VectorStringTransformer "))

  override def copy(extra: ParamMap): StringVector  = {
    defaultCopy(extra)
  }

  @Since("1.4.0")
  override def transformSchema(schema: StructType): StructType = {

    schema.add(StructField($(outputCol), new VectorUDT, false))
  }


  override def transform(df: Dataset[_]):DataFrame = {
    // 向量字符串转向量, 对于sparse 和dense 的字符串格式都支持
    val string2vector = (x: String) => {
      org.apache.spark.mllib.linalg.Vectors.parse(x).asML
    }
    val str2vec = udf(string2vector)

    // 另一种转换dense vector 字符串的方式
//    val string2vector = (x: String) => {
//      val a = StringUtils.strip(x,"[]").split(",").map(i => i.toDouble)
//      org.apache.spark.ml.linalg.Vectors.dense(a)
//    }
//    val str2vec = udf(string2vector)
//    df.withColumn($(outputCol), str2vec(df($(inputCols)(0))))
    df.withColumn($(outputCol), str2vec(col("features")))

  }


}
```

#### LRClassify

```java
package com.qianfeng.recommend

import com.qianfeng.recommend.config.Config
import com.qianfeng.recommend.transform.{LRModelData, VectorSchema}
import com.qianfeng.recommend.udfs.FeatureUDF
import com.qianfeng.recommend.util.SparkHelper
import org.apache.log4j.{Level, Logger}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
import org.apache.spark.ml.feature.StringVector
import org.apache.spark.sql.functions._
import org.slf4j.LoggerFactory

/**
  * @Description: 逻辑回归模型，对召回层物品的综合排序算法
  * @Author: QF    
  * @Date: 2020/7/29 5:05 PM   
  * @Version V1.0 
  */
object LRClassify {

  private val log = LoggerFactory.getLogger("lr-model")
  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.WARN)
    System.setProperty("HADOOP_USER_NAME", "root")

    // 解析命令行参数
    val params = Config.parseConfig(LRClassify, args)
    log.warn("job running please wait ... ")
    // init spark session
    val ss = SparkHelper.getSparkSession(params.env, "lr-model")
    // 基础数据处理
    val modelData = LRModelData(ss, params.env)
    // 使用training的数据集关用户基础向量、文章基础向量、文章嵌入向量
    val sourceData = modelData.getVectorTrainingData()

    sourceData.show(false)

    // 将article_features, user_features ,article_embedding 合并为一列
    val mergeDF = sourceData.withColumn("features",
      FeatureUDF.mergeCols(struct("user_features","article_features","article_embedding")))

    mergeDF.show(false)

    // mergeDF合并后的列features, 是一个Vector的字符串形式，将其vector每一个值转换为一个列，获取对应的schema
    // 只有生成pmml 时需要使用这个schema
    val  schema = VectorSchema.apply.getVectorSchemaByStrColumns(mergeDF,Array("features"))
    // 获取schema 对应的列名数组
    val  columns = schema.map(line=>line.name)

    // 自定义的字符串转Vector的transformer，jpmml没有这个转换，需要自定义
    val stringVector = new StringVector()
      .setInputCol(columns.toArray)  //输入列名
      .setOutputCol("features_vec")  //输出列

    // 定义逻辑回个模型
    val lr = new LogisticRegression()
      // 是否使用带截距的回归
      .setFitIntercept(true)
      // 最大迭代次数
      .setMaxIter(100)
      // 正则化系数, 值越大表示对模型训练数据集拟合系数惩罚越强,模型系数越接近或者等于0
      // 这样模型就越简单，防止模型过拟合，但越大的值可能会造成欠拟合，默认0
      .setRegParam(0)
      // 模型收敛的容忍系数,模型每次迭代比较阈值确定是否结束迭代和MaxIter参数一起控制迭代次数
      // 值越小，执行的迭代次数越多，默认值1E-6
      .setTol(1E-6)
      // 是否对输入数据做标准化处理
      .setStandardization(true)
      // 输入样本数据列
      .setFeaturesCol("features_vec")
      // 输入的标签列
      .setLabelCol("label")

    // 定义一个Pipeline,将各个特征转换及模型操作放入到其中处理
    val pipeline = new Pipeline()
      .setStages(Array(stringVector,lr))

    // 将数据集itemFeatureDF，就是我们的文章内容信息数据作用到我们定义的pipeline上
    val pipelineModel = pipeline.fit(mergeDF)

    // 获取pipeline中训练好的逻辑回归模型
    val lrModel = pipelineModel.stages(1).asInstanceOf[LogisticRegressionModel]
    // 打印模型评估指标
    modelData.printSummary(lrModel)

   /*
   //PMML模型保存
   // 从pipeline构建PMML
    val pmml = new PMMLBuilder(schema.add("label",IntegerType), pipelineModel).build
    // 打印PMML模型
//    JAXBUtil.marshalPMML(pmml, new StreamResult(System.out))
    // 保存pmml模型到文件
    val targetFile = "lr.pmml"
    val fout: StreamResult = new StreamResult( new FileOutputStream(targetFile))
    JAXBUtil.marshalPMML(pmml, fout)*/
  }
}
```



本地测试：

```properties
-e dev
```



### 模型不work分析

1、训练数据中用户、物品向量和物品被用户发生行为没有任何逻辑关系。

2、用户、物品向量数据中的特征维度较少。

3、用户、物品向量数据中的非数值类型使用one hot 编码，导致特征维度较为稀疏，可以结合GBDT提升特征维度组合的良好性。



### IRIS案例测试

#### IrisLRClassify

```java
package com.qianfeng.recommend

import java.io.FileOutputStream

import com.qianfeng.recommend.config.Config
import com.qianfeng.recommend.transform.{LRModelData, VectorSchema}
import com.qianfeng.recommend.udfs.FeatureUDF
import com.qianfeng.recommend.util.SparkHelper
import org.apache.log4j.{Level, Logger}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
import org.apache.spark.ml.feature.StringVector
import org.apache.spark.sql.functions.struct
import org.apache.spark.sql.types.IntegerType
import org.jpmml.model.JAXBUtil
import org.jpmml.sparkml.PMMLBuilder
import org.slf4j.LoggerFactory
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

/**
  * @Description: iris数据集上运行LR分类模型
  * @Author: QF    
  * @Date: 2020/8/3 10:18 PM   
  * @Version V1.0 
  */
object IrisLRClassify {

  private val log = LoggerFactory.getLogger("lr-iris")

  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.WARN)
    System.setProperty("HADOOP_USER_NAME", "root")

    // 解析命令行参数
    val params = Config.parseConfig(IrisLRClassify, args)
    log.warn("job running please wait ... ")
    // init spark session
    val ss = SparkHelper.getSparkSession(params.env, "lr-iris")
    import ss.implicits._

    val sourceData = ss.read.option("header",true).csv("hdfs://hadoop01:9000/iris.csv")
    val transDF = sourceData.withColumn("label",col("label").cast(IntegerType))
      .filter($"label"===0 || $"label" ===1)

    // 读取features列
    val mergeDF = transDF.withColumn("features",FeatureUDF.mergeCols(struct($"features")))

    mergeDF.show(false)
    // mergeDF合并后的列features, 是一个Vector的字符串形式，将其vector每一个值转换为一个列，获取对应的schema
    // 只有生成pmml 时需要使用这个schema
    val  schema = VectorSchema.apply.getVectorSchemaByStrColumns(mergeDF,Array("features"))
    // 获取schema 对应的列名数组
    val  columns = schema.map(line=>line.name)

    // 自定义的字符串转Vector的transformer，jpmml没有这个转换，需要自定义
    val stringVector = new StringVector()
      .setInputCol(columns.toArray)
      .setOutputCol("features_vec")

    // 定义逻辑回个模型
    val lr = new LogisticRegression()
      // 是否使用带截距的回归
      .setFitIntercept(true)
      // 最大迭代次数
      .setMaxIter(100)
      // 正则化系数, 值越大表示对模型训练数据集拟合系数惩罚越强,模型系数越接近或者等于0
      // 这样模型就越简单，防止模型过拟合，但越大的值可能会造成欠拟合，默认0
      .setRegParam(0)
      // 模型收敛的容忍系数,模型每次迭代比较阈值确定是否结束迭代和MaxIter参数一起控制迭代次数
      // 值越小，执行的迭代次数越多，默认值1E-6
      .setTol(1E-6)
      // 是否对输入数据做标准化处理
      .setStandardization(true)
      // 输入样本数据列
      .setFeaturesCol("features_vec")
      // 输入的标签列
      .setLabelCol("label")

    // 定义一个Pipeline,将各个特征转换及模型操作放入到其中处理
    val pipeline = new Pipeline()
      .setStages(Array(stringVector,lr))

    // 将数据集itemFeatureDF，就是我们的文章内容信息数据作用到我们定义的pipeline上
    val pipelineModel = pipeline.fit(mergeDF)

    // 获取pipeline中训练好的逻辑回归模型
    val lrModel = pipelineModel.stages(1).asInstanceOf[LogisticRegressionModel]
    // 打印模型评估指标
    val modelData = LRModelData(ss,params.env)
    modelData.printSummary(lrModel)
  }
}

```

我们接下来会使用一个小型的数据集，用同样的方式训练一个模型，让你看到一个`work的模型结果`。 我们选择在机器学习初期分类模型常用的一个数据集，`Iris` 数据集，这个是`scikit-learn`官方提供的鸢尾花数据集，其中包含了`150条`数据，分为三个类别，我们只选择中两类数据即可。 我会将样本数据以处理后CSV的方式提供给大家。我们先来看看数据大致样式

```shell
sepal_length,sepal_width,petal_length,petal_width,label,features
5.1,3.5,1.4,0.2,0.0,"[5.1,3.5,1.4,0.2]"
4.6,3.1,1.5,0.2,0.0,"[4.6,3.1,1.5,0.2]"
...

5.5,2.3,4.0,1.3,1.0,"[5.5,2.3,4.0,1.3]"
6.5,2.8,4.6,1.5,1.0,"[6.5,2.8,4.6,1.5]"
... 
6.2,3.4,5.4,2.3,2.0,"[6.2,3.4,5.4,2.3]"
5.9,3.0,5.1,1.8,2.0,"[5.9,3.0,5.1,1.8]"

# 解释一下这个数据格式，大家可以看到，数据有6列， 前4列时特征，第5列是标签，可以看到标签有 0，1，2 三类，我们只需要0，1 两类数据即可。 第6列是前四列组成的向量的字符串表示。

# 我们现在要做的就是在这个数据集上，运行我们的LR模型，来看看效果。
```

```shell
# 下载数据集到本地
wget http://doc.yihongyeyan.com/qf/project/soft/app/iris.csv
# 上传数据到HDFS目录
hdfs dfs -mkdir -p /common/data/iris/
hdfs dfs -put iris.csv /common/data/iris/
# 接下来在你的IDE运行`IrisLRClassify` 这个类就可以看到模型的训练过程了，需要的参数如下
```

```shell
IrisLRClassiy 1.0
Usage: spark  IrisLRClassify [options]

  -e, --env <value>       env: dev or prod
  -i, --irisPath <value>  irisPath: iris数据集目录
  
例子：
-e dev -i /common/data/iris/
```

```shell
# 执行训练，会看到如下类似结果
iterator: 0, loss: 0.6931471805599458
iterator: 1, loss: 0.5627324422013428
iterator: 2, loss: 0.34456217377333004
iterator: 3, loss: 0.11092422599831109
iterator: 4, loss: 0.06346920375678597
iterator: 5, loss: 0.032063511506252126
iterator: 6, loss: 0.017514452996952826
iterator: 7, loss: 0.00949745051061229
iterator: 8, loss: 0.005224070638124724
iterator: 9, loss: 0.002845324292301522
iterator: 10, loss: 0.0015270593938253225
iterator: 11, loss: 8.076381951314593E-4
iterator: 12, loss: 4.517497558911552E-4
iterator: 13, loss: 3.638741976662672E-4
iterator: 14, loss: 1.0060488894802138E-4
iterator: 15, loss: 6.82457799397856E-5
iterator: 16, loss: 3.5156853862672354E-5
iterator: 17, loss: 2.0463274122188138E-5
iterator: 18, loss: 1.1399719621275049E-5
iterator: 19, loss: 6.40229581372871E-6
iterator: 20, loss: 3.4952981000444156E-6
iterator: 21, loss: 1.8015418777203539E-6
iterator: 22, loss: 9.003246144361831E-7
iterator: 23, loss: 4.5364447339731775E-7
iterator: 24, loss: 2.2771891947077218E-7
iterator: 25, loss: 1.1550073784374986E-7
iterator: 26, loss: 9.289757235051972E-8
iterator: 27, loss: 4.6917958621276634E-8
iterator: 28, loss: 2.4404903940787078E-8
iterator: 29, loss: 1.2068362760084395E-8
iterator: 30, loss: 6.079302429282733E-9
accuracy:1.0
auc: 1.0

# 你会看到loss 逐渐减低，并且很快就趋近于0, 迭代到30次，损失已经是 6.07E-9。 最后模型的精度是1，auc也是1，说明模型训练的特别好，在我们训练数据集上能到100%正确。
```



# day03作业

1、理解为啥需要用户基础特征、物品特征和文章嵌入

2、将LR模型保存成PMML文件



## 2.10 PMML

`PMML(Predictive Model Markup Language)` 预测模型标记语言。就是将模型使用xml(描述)文件保存到本地。主要用于：模型可以被几乎任何的语言调用(java、python)。



优点：

1、跨平台运行，多平台只需要写一次模型即可。

2、轻量级模型加载，因为只需要加载一个pmml模型文件即可。

3、模型可以融合到支持高并发，毫秒级的延迟的应用中。



缺点：

pmml有的语言不提供类库；；

有的类库支持不是很好，比如jpmml对字符串向量支持就不好，需要自定义。





```

```

### Jpmml自定义转换

[jpmml-sparkml](https://github.com/jpmml/jpmml-sparkml)这个类库有些`transformer`是不支持的，比如我们需要用到的将`string vector` 转换为`vector` 的`transformer`它就不支持，需要自己定义向量转字符串和字符串转向量。

错误信息如下：

```java
Transformer class org.apache.spark.ml.feature.StringVector is not supported

解决方法：
自定义jpmml支持的StringVector
```



```

```





### Union-Feature

我们`recommend:news-cf` 表存储了`ItemCF`,`ALS` 两个推荐算法产生的结果，也就是两路召回的结果。 我们要做的是，一个用户请求过来，我们从这两路召回中取出一定量的数据(比如都取5条数据)， 之后对取出来的数据通过我们训练好的模型做排序输出。 模型的输入是`recommend:news-feature` 表中的文章稀疏特征向量和文章的Embedding向量，以及`recommend:user-feature` 表中的用户稀疏特征向量。 模型输出的结果就是每个物品被点击的概率，我们按照这个概率降序排序就可以了。看起来很美好，但是工程实现的时候是不能直接这么做的，因为，你要想为一个用户准备好模型需要的特征数据，需要多次访问HBASE，尤其是当从一个召回策略中获取到推荐的物品后，你还需要遍历每个物品，然后根据物品ID去请求HBASE的物品特征表，获取特征向量，这无疑是非常低效的，那有什么方式解决呢，方法也很简单，召回策略产生物品推荐列表时和物品特征向量做关联，将物品特征向量拼接到推荐物品列表上，同时将用户特征向量也融入到召回表的一个列中。 这样我们只需要请求一次HBASE即可。 我们融合后的数据格式应当如下：

```markdown
表名： recommend:union-feature
行键：uid
列族：f1
列1： als  值: item1:score:item1_vector:item1_embedding,item2:score:item2_vector:item2_embedding
列2:	itemcf 值： item1:score:item1_vector:item1_embedding,item3:score:item3_vector:item3_embedding
列3： uf 值: user_vector

# 解释一下
用户ID作为行键 

列1存储als推荐算法产生的推荐结果列表，每个推荐的物品拼接上该物品的稀疏特征向量和Embedding向量。例如 `item1:score:item1_vector:item1_embedding` 
item1 表示一个物品
score 表示这个物品在als算法下的评分
item1_vector 表示这个物品的稀疏特征向量
item1_embedding 表示这个物品的embeding向量

列2存储itemcf算法结果，格式和列1一致

列3存储该用户的稀疏特征向量
```

```shell
# 执行命令在HBASE创建这个表
create 'recommend:union-feature','f1'
```

> 那接下来我们就写程序实现这个过程吧，关键代码如下，[完整代码参见](

#### UnionFeatureModelData

```java
package com.qianfeng.recommend


import com.qianfeng.recommend.config.Config
import com.qianfeng.recommend.hbase.HBaseUtil
import com.qianfeng.recommend.transform.UnionFeatureModelData
import com.qianfeng.recommend.util.SparkHelper
import org.apache.log4j.{Level, Logger}
import org.slf4j.LoggerFactory

/**
  * @Description: 合并召回算法的用户及物品特征存储到HBASE
  * @Author: QF    
  * @Date: 2020/8/5 6:20 PM   
  * @Version V1.0 
  */
object UnionFeature {

  private val log = LoggerFactory.getLogger("union-feature")

  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.WARN)
    System.setProperty("HADOOP_USER_NAME", "root")

    // 解析命令行参数
    val params = Config.parseConfig(UnionFeature, args)
    log.warn("job running please wait ... ")
    // init spark session
    val ss = SparkHelper.getSparkSession(params.env, "union-feature")
    val unionData = UnionFeatureModelData(ss, params.env)

    // 关联用户特征  用户id、user_base_vec
    val userFeatureDF = unionData.genUserFeature()
    userFeatureDF.show(false)

    val userFeatureHFileRDD = unionData.userFeaturesDF2HFile(userFeatureDF,"uf")

    log.warn("load user feature result to hbase ! ")
    // 将特征向量存入到HBASE,先将其换换为HFile
    val hBaseUtil = HBaseUtil(ss, params.zkHost, params.zkPort)
    hBaseUtil.loadHfileRDD2Hbase(userFeatureHFileRDD, params.htableName, params.hfilePath)

    // itemcf 关联文章特征 uid,sim_aid,pred_rate,features,embedding
    val itemCFFeature = unionData.genItemCFFeature()
    itemCFFeature.show(false)
    val itemCFConvert = unionData.featureDataConvert(itemCFFeature)
    val itemCFHFileRDD = unionData.featuresDF2HFile(itemCFConvert, "itemcf")

    // HFile RDD 生成文件后直接加载到HBASE中
    log.warn("load itemcf feature result to hbase ! ")
    hBaseUtil.loadHfileRDD2Hbase(itemCFHFileRDD, params.htableName, params.hfilePath)

    // als 关联文章特征
    val alsFeature = unionData.genALSFeature()
    alsFeature.show(false)
    val alsConvert = unionData.featureDataConvert(alsFeature)
    val alsHFileRDD = unionData.featuresDF2HFile(alsConvert, "als")
    // HFile RDD 生成文件后直接加载到HBASE中
    log.warn("load als feature result to hbase ! ")
    hBaseUtil.loadHfileRDD2Hbase(alsHFileRDD, params.htableName, params.hfilePath)

    ss.stop()
    log.warn("job success! ")
  }
}
```

#### UnionFeature

```java
package com.qianfeng.recommend


import com.qianfeng.recommend.config.Config
import com.qianfeng.recommend.hbase.HBaseUtil
import com.qianfeng.recommend.transform.UnionFeatureModelData
import com.qianfeng.recommend.util.SparkHelper
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql
import org.apache.spark.sql.DataFrame
import org.slf4j.LoggerFactory

/**
  * @Description: 合并召回算法的用户及物品特征存储到HBASE
  * @Author: QF    
  * @Date: 2020/8/5 6:20 PM   
  * @Version V1.0 
  */
object UnionFeature {

  private val log = LoggerFactory.getLogger("union-feature")

  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.WARN)
    System.setProperty("HADOOP_USER_NAME", "root")

    // 解析命令行参数
    val params = Config.parseConfig(UnionFeature, args)
    log.warn("job running please wait ... ")
    // init spark session
    val ss = SparkHelper.getSparkSession(params.env, "union-feature")
    val unionData = UnionFeatureModelData(ss, params.env)

    // 关联用户特征  用户id、user_base_vec
    val userFeatureDF = unionData.genUserFeature()
    userFeatureDF.show(false)

    val userFeatureHFileRDD = unionData.userFeaturesDF2HFile(userFeatureDF,"uf")

    log.warn("load user feature result to hbase ! ")
    // 将特征向量存入到HBASE,先将其换换为HFile
    val hBaseUtil = HBaseUtil(ss, params.zkHost, params.zkPort)
    hBaseUtil.loadHfileRDD2Hbase(userFeatureHFileRDD, params.htableName, params.hfilePath)

    // itemcf 关联文章特征 uid,sim_aid,pred_rate,features,embedding
    val itemCFFeature = unionData.genItemCFFeature()
    itemCFFeature.show(false)
    val itemCFConvert = unionData.featureDataConvert(itemCFFeature)
    val itemCFHFileRDD = unionData.featuresDF2HFile(itemCFConvert, "itemcf")

    // HFile RDD 生成文件后直接加载到HBASE中
    log.warn("load itemcf feature result to hbase ! ")
    hBaseUtil.loadHfileRDD2Hbase(itemCFHFileRDD, params.zkHost, params.hfilePath)

    // als 关联文章特征
    val alsFeature = unionData.genALSFeature()
    alsFeature.show(false)
    val alsConvert = unionData.featureDataConvert(alsFeature)
    val alsHFileRDD = unionData.featuresDF2HFile(alsConvert, "als")
    // HFile RDD 生成文件后直接加载到HBASE中
    log.warn("load als feature result to hbase ! ")
    hBaseUtil.loadHfileRDD2Hbase(alsHFileRDD, params.zkHost, params.hfilePath)

    ss.stop()
    log.warn("job success! ")
  }
}

```

本地测试：

```properties
-e dev -h hadoop01 -p 2181 -t recommend:union-feature -f /recommend/union
```



### Spring Boot PMML API

有了`union-feature`我们接下来就通过`SpringBoot` 加载我们的模型，使用`union-feature`特征作为模型的输入进行排序计算。我们列一下实现步骤

1. 将训练生成的模型文件`lr.pmml`，添加到`data-api` 项目的`resources`目录下

2. 添加`pmml-evaluator` pom依赖库，这个库加载模型文件做预测

3. 从`hbase中的 recommend:union-feature`表中获取`itemcf和als`这两类召回算法生成的数据及相关的特征数据， 送入排序模型进行预测。 这里注意两个问题

   ```markdown
   # 我们代码中会将这两路召回算法为用户推荐的物品全部取出来，每个物品并行的送入到`pmml`模型获取预测值，然后根据预测值排序，返回推荐结果。 但是将一个用户的每个召回算法的结果都取出来，做排序是不现实的同时也没有必要的，因为推荐结果展示给用户是，一般返回20~30条结果就可以了，用户每次刷新再词请求推荐接口即可。 这里我们只有两路召回，事实上我们可以有N路召回，每路召回结果里面可能存储了 100~1000条数据，我们也不可能将所有的召回结果都取出来的。 这里可以设计一个参数，可以控制每一路召回取数据的比例，比如用户每次请求要返回30条数据，那这个参数如果是 1:1 表示，每路召回结果返回15条。同时有了这个参数我们也能灵活控制召回策略的比例和策略的上下线。
   # 我们不同召回模型产生的推荐物品列表，是有可能重复的，多路召回结果放入预测模型之前，最好根据物品ID去重
   
   * 以上两个功能有大家可以自己尝试实现，我们不提供代码
   ```

#### pom.xml

```xml
 <!--引入pmml的依赖和扩展-->
<dependency>
    <groupId>org.jpmml</groupId>
    <artifactId>pmml-evaluator</artifactId>
    <version>1.5.1</version>
</dependency>
<dependency>
    <groupId>org.jpmml</groupId>
    <artifactId>pmml-evaluator-extension</artifactId>
    <version>1.5.1</version>
</dependency>

<!--引入prometheus的监控依赖-->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
<dependency>
    <groupId>io.micrometer</groupId>
    <artifactId>micrometer-registry-prometheus</artifactId>
</dependency>
<!--hbase的客户端依赖-->
<dependency>
    <groupId>org.apache.hbase</groupId>
    <artifactId>hbase-shaded-client</artifactId>
    <version>1.3.6</version>
</dependency>
```





#### 控制器

```java
package com.qianfeng.controller;

import com.alibaba.fastjson.JSONArray;
import com.alibaba.fastjson.JSONObject;
import com.alibaba.fastjson.JSONPath;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.web.bind.annotation.*;

import java.util.List;

/**
 * @Description: 推荐服务接口
 * @Author: QF
 * @Date: 2020/6/26 10:32 PM
 * @Version V1.0
 */
@RestController
@SpringBootApplication
@RequestMapping(value = "api/v1/")
public class RecommendCtrl {

    @Autowired
    private RecommendService rs;


    /**
     * 指定用户ID, 给用户推荐物品，两路 召回+ LR(PMML)排序模型
     * @param uid
     * @return
     */
    @RequestMapping(value = "recommend/{uid}", method = {RequestMethod.GET})
    @ResponseBody
    public RecommendResult recommend(@PathVariable String uid) {
        return rs.recommend(uid);

    }

}

```



#### 推荐结果封装

#### RecommendInfo

```java
package com.qianfeng.bean;

/**
 * @Description: 查询结果封装，表是推荐的物品ID和ID对应排序分数
 * @Author: QF
 * @Date: 2020/7/10 12:07 PM
 * @Version V1.0
 */
public class RecommendInfo {
    private int aid;
    private Double probability;

    public int getAid() {
        return aid;
    }

    public void setAid(int aid) {
        this.aid = aid;
    }

    public Double getProbability() {
        return probability;
    }

    public void setProbability(Double probability) {
        this.probability = probability;
    }
}

```

#### RecommendResult

```java
package com.qianfeng.bean;

import java.util.List;

/**
 * @Description: user recommend 查询返回的结果
 * @Author: QF
 * @Date: 2020/6/26 10:57 PM
 * @Version V1.0
 * {
 *     "code":200,
 *     "msg":"推荐成功",
 *     "data":[{"aid",123,"probability":0.9232},{"aid",234,"probability":0.9201},,,]
 * }
 */
public class RecommendResult {

    private int code; // 返回状态吗
    private String msg; // 与状态对应的信息

    private List<RecommendInfo> data;

    public RecommendResult() {
    }

    public int getCode() {
        return code;
    }

    public void setCode(int code) {
        this.code = code;
    }

    public String getMsg() {
        return msg;
    }

    public void setMsg(String msg) {
        this.msg = msg;
    }

    public List<RecommendInfo> getData() {
        return data;
    }

    public void setData(List<RecommendInfo> data) {
        this.data = data;
    }
}
```



#### 接口

```java
package com.qianfeng.service;


import com.qianfeng.bean.RecommendResult;


/**
 * @Description:  从clickhouse 查询用户标签查询
 * @Author: QF
 * @Date: 2020/6/25 12:09 PM
 * @Version V1.0
 */
public interface RecommendService {
    // 用户推荐接口
    RecommendResult recommend(String uid);
}

```



### HBase工具

#### HbaseProperties

```java
package com.qianfeng.repostory;

import org.springframework.boot.context.properties.ConfigurationProperties;


/**
*@Author 东哥
*@Company 千锋好程序员大数据
*@Date
*@Description通过application.yml中的数据源注入hbase连接信息
**/
@ConfigurationProperties(prefix = "spring.data.hbase")
public class HbaseProperties {

    private String quorum;

    private String rootDir;

    private String nodeParent;

    public String getQuorum() {
        return quorum;
    }

    public void setQuorum(String quorum) {
        this.quorum = quorum;
    }

    public String getRootDir() {
        return rootDir;
    }

    public void setRootDir(String rootDir) {
        this.rootDir = rootDir;
    }

    public String getNodeParent() {
        return nodeParent;
    }

    public void setNodeParent(String nodeParent) {
        this.nodeParent = nodeParent;
    }
}
```

#### 配置数据源：

```java
    hbase:
      quorum: hadoop01:2181
      rootDir: /hbase
      nodeParent: /
```

#### HbaseAutoConfiguration

```java
package com.qianfeng.repostory;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.autoconfigure.EnableAutoConfiguration;
import org.springframework.boot.context.properties.EnableConfigurationProperties;
import org.springframework.context.annotation.Bean;


@org.springframework.context.annotation.Configuration
@EnableAutoConfiguration
@EnableConfigurationProperties(HbaseProperties.class)
//@ConditionalOnClass(HbaseTemplate.class)
public class HbaseAutoConfiguration {

    private static final String HBASE_QUORUM = "hbase.zookeeper.quorum";
    private static final String HBASE_ROOTDIR = "hbase.rootdir";
    private static final String HBASE_ZNODE_PARENT = "zookeeper.znode.parent";

    //根据类型将HbaseProperties对象注解进来
    @Autowired
    private HbaseProperties hbaseProperties;

    //为hbase设置对应的属性
    @Bean
//    @ConditionalOnMissingBean(HbaseTemplate.class)
    public HbaseTemplate hbaseTemplate() {
        Configuration configuration = HBaseConfiguration.create();
        configuration.set(HBASE_QUORUM, this.hbaseProperties.getQuorum());
        configuration.set(HBASE_ROOTDIR, hbaseProperties.getRootDir());
        configuration.set(HBASE_ZNODE_PARENT, hbaseProperties.getNodeParent());
        return new HbaseTemplate(configuration);
    }
}
```

#### HbaseTemplate

```java
package com.qianfeng.repostory;


import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;

/*
* @Description
* @Author hbasetempet模板
* @param null
* @Return
* @Exception
*
*/
public class HbaseTemplate {

    private static final Logger LOGGER = LoggerFactory.getLogger(HbaseTemplate.class);

    private Configuration configuration;

    private volatile Connection connection;

    public HbaseTemplate() {
    }

    public HbaseTemplate(Configuration configuration) {
        this.setConfiguration(configuration);
    }

    public Configuration getConfiguration() {
        return configuration;
    }

    public void setConfiguration(Configuration configuration) {
        this.configuration = configuration;
    }

    //获取hbase的连接
    public Connection getConnection() {
        if (null == this.connection) {
            synchronized (this) {
                if (null == this.connection) {
                    try {
                        this.connection = ConnectionFactory.createConnection(configuration);
                        LOGGER.info("hbase connection:"+ this.connection);
                    } catch (IOException e) {
                        LOGGER.error("hbase connection创建失败");
                    }
                }
            }
        }
        return this.connection;
    }
}
```

#### HbaseModelFeatureQuery

```java
package com.qianfeng.repostory;

import org.apache.commons.lang3.StringUtils;
import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.CellUtil;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.Table;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;

import javax.annotation.PostConstruct;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

/**
 * @Description: 查询hbase中存储的特征数据
 * @Author: QF
 * @Date: 2020/8/6 10:56 AM
 * @Version V1.0
 */
@Component
public class HbaseModelFeatureQuery {

    private static final Logger logger = LoggerFactory.getLogger(HbaseModelFeatureQuery.class);
    @Autowired
    private HbaseTemplate hbaseTemplate;

    private static HbaseTemplate hbaseTemplateStatic;

    private static HbaseModelFeatureQuery hbaseModeFeatureQuery;

    static String FEATURE_TABLE_NAME = "recommend:union-feature";

    //初始化
    @PostConstruct
    public void init(){
        hbaseModeFeatureQuery = this;
        hbaseModeFeatureQuery.hbaseTemplateStatic=this.hbaseTemplate;
        hbaseTemplateStatic.getConnection();
    }

    //从Hbase union-feature 取出所有的列column-value 转换成map=> key->column value->value
    public static Map<String,String> parseFeatures(String uid) {
        Map<String,String> map = new HashMap<String,String>();
        try {
            //将用户id传入封装hbase的Get对象中
            Get get = new Get(uid.getBytes());
            Table htable = hbaseTemplateStatic.getConnection().getTable(TableName.valueOf(FEATURE_TABLE_NAME));
            //获取某个用户的结果数据
            Result rs = htable.get(get);
            //对rs结果进行判断
            if(rs.isEmpty()) {
                logger.error("user not exist" + ":" + uid);
                return null;
            }else {
                Cell[] cells = rs.rawCells();
                for(Cell cell : cells ) {
                    //获取hbase中某行的列
                    String qualifier = new String(CellUtil.cloneQualifier(cell));
                    //获取hbase中某行的value
                    String value = new String(CellUtil.cloneValue(cell),"UTF-8");
                    //value不等于空
                    if(!"".equals(value)) {
                        map.put(qualifier, value);
                    }
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
        return map;
    }

    // 将从hbase取出的特征数据，转换为每个物品列表及其对应的相关向量
    // 注意这里将所有的als和itemcf算法产生的结果都取出来了，实际情况下我们应该每个算法里面取topk,然后
    // 对topk的数据排序，同时取过的数据，下次就不能再使用了，并且每个算法取的数据比例是可控的，这个逻辑
    // 需要大家自己实现
    public static Map<String,String> transItemFeatureList(String uid) {
        //解析用户特征
        Map<String,String> parseFeatures = parseFeatures(uid);
        if (parseFeatures==null){
            return null;
        }

        //定义kve-value
        Map<String,String> itemFeatureMap = new HashMap<String,String>();
        String userVector = parseFeatures.get("uf"); //取出用户基础特征向量
        String als = parseFeatures.get("als"); //取出als召回算法推荐物品列表及物品基础向量、物品嵌入向量
        String itemcf = parseFeatures.get("itemcf");
        //任何一列值不能为空
        if (itemcf==null|| userVector==null|| als==null){
            logger.error("user info [uf,als,itemcf] some features is null");
            return null;
        }
        //uf,als,itemcf三个向量都不为空
        String[]  alsItemArray = als.split(";");  //拆分每一个item(文章)
        for (String alsItem :alsItemArray){
            String[] alsInfo = alsItem.split(":"); //拆分：itemId:rating:基础向量:文章嵌入向量
            String alsItemID = alsInfo[0];
            String alsItemVector = alsInfo[2];
            String alsItemEmbedding = alsInfo[3];
            String unionFeature = StringUtils.strip(alsItemVector,"[]")+","+
                    StringUtils.strip(alsItemEmbedding,"[]")+","+
                    StringUtils.strip(userVector,"[]");
            itemFeatureMap.put(alsItemID,unionFeature);
        }

        //基于ItemCF的推荐的物品及向量
        String[]  itemcfArray = itemcf.split(";");
        for (String itemcfItem :itemcfArray){
            String[] itemcfInfo = itemcfItem.split(":");
            String itemcfItemID = itemcfInfo[0];
            String itemcfItemVector = itemcfInfo[2];
            String itemcfItemEmbedding = itemcfInfo[3];
            String unionFeature = StringUtils.strip(itemcfItemVector,"[]")+","+
                    StringUtils.strip(itemcfItemEmbedding,"[]")+","+
                    StringUtils.strip(userVector,"[]");
            itemFeatureMap.put(itemcfItemID,unionFeature);
        }
        return itemFeatureMap;
    }

}
```

#### LRModelPredict

```java
package com.qianfeng.repostory;

import com.alibaba.fastjson.JSONObject;
import org.dmg.pmml.FieldName;
import org.jpmml.evaluator.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.core.io.ClassPathResource;
import org.springframework.stereotype.Component;
import org.xml.sax.SAXException;

import javax.annotation.PostConstruct;
import javax.xml.bind.JAXBException;
import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;

/**
 * @Description:
 * @Author: QF
 * @Date: 2020/8/3 5:07 PM
 * @Version V1.0
 *  使用LR模型为推荐的每一个物品进行点击率的预测
 */

@Component
public class LRModelPredict {

    private static Logger logger = LoggerFactory.getLogger(LRModelPredict.class);
    private static Evaluator evaluator;

    //初始化LR模型的计算器
    @PostConstruct
    public void init() {
        //使用类加载器--加载pmml模型
        ClassPathResource classPathResource = new ClassPathResource("lr.pmml");
        try {
            InputStream inputStream = classPathResource.getInputStream();
            //将模型还原成计算器
            evaluator = new LoadingModelEvaluatorBuilder().load(inputStream).build();
        } catch (IOException e) {
            e.printStackTrace();
        } catch (SAXException e) {
            e.printStackTrace();
        } catch (JAXBException e) {
            e.printStackTrace();
        }
        evaluator.verify();
    }


    /**
     * 传入特征给模型进行预测，特征格式为逗号分隔的特征值
     *
     * @param feature 格式为 逗号分隔的特征值
     * @return
     */
    public static Double predictProbability(String feature) {
        // 获取模型定义的特征字段
        List<? extends InputField> inputFields = evaluator.getInputFields();

        //将传入进来的向量进行拆分
        String[] featureArray = feature.split(","); //输入向量

        //判断特征字段个数是否和输入的字段数量不等
        if (inputFields.size() != featureArray.length) {
            logger.error(String.format("model input feature size error, need features size %s ,but %s",
                    inputFields.size(), featureArray.length));
            return -1.0;
        }

        //使用有序的Map
        Map<FieldName, FieldValue> arguments = new LinkedHashMap<>();
        int count = 0;
        // 变量模型的特征字段，给每个特征字段赋值
        for (InputField inputField : inputFields) {
            //取出对应特征字段名称
            FieldName inputName = inputField.getName();
            Object rawValue = featureArray[count];
            count = count + 1;
            //为对应的特征字段赋值
            FieldValue inputValue = inputField.prepare(rawValue);
            arguments.put(inputName, inputValue);
        }

        // 预测结果
        Map<FieldName, ?> results = evaluator.evaluate(arguments);
        // 解析模型预测结果
        Map<String, ?> resultRecord = EvaluatorUtil.decodeAll(results);
//        logger.info(resultRecord.toString());
        String label = resultRecord.get("label").toString();
        // 获取概率
        String probability = resultRecord.get(String.format("probability(%s)", label)).toString();

        return Double.valueOf(probability); //将概率转换成double类型
    }


    /**
     * 支持传入JSON 格式的特征
     *
     * @param feature json 格式特征
     * @return
     */
    public static Double predictProbabilityByJson(JSONObject feature) {
        // 获取模型定义的特征
        List<? extends InputField> inputFields = evaluator.getInputFields();

        Map<FieldName, FieldValue> arguments = new LinkedHashMap<>();
        for (InputField inputField : inputFields) {
            FieldName inputName = inputField.getName();
            String name = inputName.getValue();
            Object rawValue = feature.getDoubleValue(name);
            FieldValue inputValue = inputField.prepare(rawValue);
            arguments.put(inputName, inputValue);
        }

        Map<FieldName, ?> results = evaluator.evaluate(arguments);
        Map<String, ?> resultRecord = EvaluatorUtil.decodeAll(results);
        logger.info(resultRecord.toString());
        String label = resultRecord.get("label").toString();
        String probability = resultRecord.get(String.format("probability(%s)", label)).toString();

        return Double.valueOf(probability);
    }
}

```

#### RecommendServiceImpl

```java
package com.qianfeng.service.impl;


import com.qianfeng.bean.RecommendInfo;
import com.qianfeng.bean.RecommendResult;
import com.qianfeng.repostory.HbaseModelFeatureQuery;
import com.qianfeng.repostory.LRModelPredict;
import com.qianfeng.service.RecommendService;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;

/**
 * @Description: 推荐服务接口
 * @Author: QF
 * @Date: 2020/7/30 9:59 AM
 * @Version V1.0
 */

@Service
public class RecommendServiceImpl implements RecommendService {
    private static final Logger logger = LoggerFactory.getLogger(RecommendServiceImpl.class);

    //定义执行线程池 --- 线程数量60个
    private ExecutorService pool = Executors.newFixedThreadPool(60);

    /**
     * 根据用户ID，查询HBASE中存储的两路[ALS和Itemc]召回策略结果，并将召回结果，根据从HBASE获取的相关用户及物品特征向量
     * 送入到排序模型进行点击率预测，对每个物品送入到排序模型使用多线程方式，根据打分结果从高到低排序输出。
     * 需要注意的是：
     * 当前这个方法将所有的als和itemcf算法产生的结果都取出来了，实际情况下我们应该每个算法里面取一定
     * 数量[数量和每个算法取数的比例应该是可控的参数],然后对一定数据的物品排序，同时取过的数据，下次就不能再使用了个逻辑
     * 大家可以自己思考实现
     *
     * @param uid 用户ID
     * @return 获取的推荐列表
     */
    @Override
    public RecommendResult recommend(String uid) {
        //初始化推荐结果对象
        RecommendResult result = new RecommendResult();
        result.setCode(0);
        result.setMsg("ok");

        // 从HBASE 中读取特征数据并转换格式
        // key:给uid推荐(ItemCF/ALS)文章Id
        // value:文章基础向量,文章嵌入向量,用户基础向量
        Map<String, String> featureMap = HbaseModelFeatureQuery.transItemFeatureList(uid);

        if (featureMap == null) {
            String msg = String.format("user %s : not exists or  user some info is null", uid);
            logger.error(msg);
            result.setCode(-1);
            result.setMsg(msg);
            result.setData(null);
            return result;
        }

        // 多线程请求预测模型，并获取每个物品的预测结果
        ArrayList<Future<String>> futures = new ArrayList<Future<String>>();
        for (Map.Entry<String, String> entry : featureMap.entrySet()) {
            //获取模型预测实例
            ModelPredict mp = new ModelPredict(entry.getKey() + ":" + entry.getValue());
           //将预测模型加入到线程池中
            Future<String> f = pool.submit(mp);
            futures.add(f);
        }

        //将每个线程执行的结果循环放到list中
        List<String> preditRes = new ArrayList<String>();
        for (Future<String> f : futures) {
            try {
                preditRes.add(f.get());
            } catch (Exception e) {
                logger.error("get predict value error", e);
            }
        }


        // 按照预测结果的可能性数值从高到低排序
        Collections.sort(preditRes, new Comparator<String>() {
            @Override
            public int compare(String o1, String o2) {
                //o1=aid:预测的点击率  o2=aid:预测的点击率
                Double prob1 = Double.valueOf(o1.split(":")[1]);
                Double prob2 = Double.valueOf(o2.split(":")[1]);
                if (prob1 == prob2) {
                    return 0;
                } else {
                    return prob2 > prob1 ? 1 : -1;
                }
            }

        });

        // 封装推荐结果
        List<RecommendInfo> recoInfoList = new ArrayList<RecommendInfo>();
        //循环排序之后的列表(aid:预测点击率)
        for (String preditItem : preditRes) {
            RecommendInfo ri = new RecommendInfo();
            ri.setAid(Integer.valueOf(preditItem.split(":")[0]));
            ri.setProbability(Double.valueOf(preditItem.split(":")[1]));
            recoInfoList.add(ri);
        }
        //设置推荐结果列表到返回对象中
        result.setData(recoInfoList);
        return result;
    }


    //模型预测封装
    private class ModelPredict implements Callable<String> {
        private String feature;

        public ModelPredict(String feature) {
            this.feature = feature;
        }

        @Override
        public String call() {
            String[] fArr = feature.split(":");
            String itemId = fArr[0];  //推荐文章id
            String itemFeature = fArr[1]; //推荐的文章基础向量,文章嵌入向量,用户基础向量
            //预测点击可能性
            Double value = LRModelPredict.predictProbability(itemFeature);
            return itemId + ":" + value;
        }
    }
}
```



### 推荐API服务

请求：http://localhost:7088/api/v1/recommend/2992

![1604281054090](%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/1604281054090.png)



所有预测点击率为-1，则代表LR模型不worker。

原因：

```
特征数据有问题：训练数据有问题

解决方法：
检查每个特征数据
```

请求：http://localhost:7088/api/v1/recommend/2992

![1604282499593](%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/1604282499593.png)



### 监控

经过前几个项目的学习，大家应该对如何使用`prometheus+grafana` 做监控已经比较熟悉了。本项目的中的监控，就简单说明了, 我们有两个需求

1. 监控一下`Spring Boot API` 的请求情况接口的`QPS，延时等`，以及其`JVM`相关信息。
2. 监控HBASE相关指标

```markdown
# 对于第一个需求，非常简单
1. 给SpringBoot项目添加如下依赖
      <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-actuator</artifactId>
      </dependency>
      <dependency>
        <groupId>io.micrometer</groupId>
        <artifactId>micrometer-registry-prometheus</artifactId>
      </dependency>
      
2. application.yml 添加如下配置，即可暴露Prometheus metric接口
management:
  endpoints:
    web:
      exposure:
        include: 'prometheus'   
        
 3. promethus.yml添加如下配置
 
   - job_name: 'my-spring-boot'
    scrape_interval: 5s
    metrics_path: /actuator/prometheus
    static_configs:
    - targets: ['192.168.216.111:7088']
    
    
4. 启动项目，访问 localhost:prot/actuator/prometheus 就可以看到metric信息，将此接口配置到prometheus中，然后再Grafana，使用`4701` 这个dashboard ID即可。 自己操作，不再赘述。

  
#  对于第二需求，有兴趣的同学自己探索吧。 
#  此外你还可以使用Prometheus的JAVA API在你的项目自定义任何你认为重要的指标，知道这个事情就行。
```



请求：http://192.168.216.111:7088/actuator/prometheus

![1604283982980](%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/1604283982980.png)





启动Grafana,然后使用`4701` 这个dashboard ID即可。 自己操作，不再赘述。

结果如下：

![1604284433939](%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/1604284433939.png)